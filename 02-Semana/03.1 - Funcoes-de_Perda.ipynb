{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03.1 - Funcoes-de_Perda.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ed03SC1Jm9Yy","colab_type":"text"},"source":["# Funções de perda (*loss functions*)\n","\n","Neste código iremos analisar diferentes funções de perda (também conhecidas como *loss functions*) que são usadas para avaliar a rede no estado atual.\n","\n","Funções de perda, também conhecidas como *loss functions*, são muito importantes para o aprendizagem de máquinas, pois servem como uma forma de medir a distância ou a diferença entre a saída prevista de um modelo e o seu valor real, auxiliando então no treino no modelo.\n","\n","Diversas funções de perda foram propostas ao longo do tempo para diferentes tipos de problemas.\n","Algumas dessas funções foram propostas para auxiliar no treino de modelos de regressão linear, como as *loss* [L1](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.L2Loss), [L2](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.L1Loss) e [Huber](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.HuberLoss).\n","Outras foram propostas para serem usadas em problemas de classificação, como a mais comum de todas [Cross-Entropy](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss).\n","\n","<p align=\"center\">\n","  <img src=\"https://drive.google.com/uc?export=view&id=1ITV4Ikw0NP39p1KNFkt46LWwnhzLKf7h\">\n","</p>\n"]},{"cell_type":"markdown","metadata":{"id":"Gp6CwWnFnTwb","colab_type":"text"},"source":["Esse pequeno bloco de código abaixo é usado somente para instalar o MXNet para CUDA 10. Execute esse bloco somente uma vez e ignore possíveis erros levantados durante a instalação.\n","\n","**ATENÇÃO: a alteração deste bloco pode implicar em problemas na execução dos blocos restantes!**"]},{"cell_type":"code","metadata":{"id":"XW-VATPAldgt","colab_type":"code","outputId":"742dac37-d016-4707-cd29-5e2259dca649","executionInfo":{"status":"ok","timestamp":1562159567189,"user_tz":180,"elapsed":53336,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":641}},"source":["!pip install mxnet-cu100\n","\n","# imports basicos\n","import time, os, sys, numpy as np\n","import mxnet as mx\n","from mxnet import autograd, gluon, init, nd\n","from mxnet.gluon import loss as gloss, nn, utils as gutils, data as gdata\n","from sklearn.model_selection import train_test_split\n","\n","# Tenta encontrar GPU\n","def try_gpu():\n","    try:\n","        ctx = mx.gpu()\n","        _ = nd.zeros((1,), ctx=ctx)\n","    except mx.base.MXNetError:\n","        ctx = mx.cpu()\n","    return ctx\n","\n","ctx = try_gpu()\n","ctx"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting mxnet-cu100\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/91/b5c2692297aa5b8c383e0da18f9208fc6d5519d981c03266abfbde897c41/mxnet_cu100-1.4.1-py2.py3-none-manylinux1_x86_64.whl (488.3MB)\n","\u001b[K     |████████████████████████████████| 488.3MB 35kB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (2.21.0)\n","Collecting graphviz<0.9.0,>=0.8.1 (from mxnet-cu100)\n","  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n","Collecting numpy<1.15.0,>=1.8.2 (from mxnet-cu100)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c4/395ebb218053ba44d64935b3729bc88241ec279915e72100c5979db10945/numpy-1.14.6-cp36-cp36m-manylinux1_x86_64.whl (13.8MB)\n","\u001b[K     |████████████████████████████████| 13.8MB 26.3MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2019.6.16)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (3.0.4)\n","\u001b[31mERROR: spacy 2.1.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fastai 1.0.54 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: blis 0.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: graphviz, numpy, mxnet-cu100\n","  Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","  Found existing installation: numpy 1.16.4\n","    Uninstalling numpy-1.16.4:\n","      Successfully uninstalled numpy-1.16.4\n","Successfully installed graphviz-0.8.4 mxnet-cu100-1.4.1 numpy-1.14.6\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["gpu(0)"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"g9u0pCOtlWLu","colab_type":"code","colab":{}},"source":["## carregando dados básicos\n","\n","# dados sintéticos somente para \n","def synthetic_regression_data(w, b, num_examples):\n","    \"\"\"generate y = X w + b + noise\"\"\"\n","    X = nd.random.normal(scale=1, shape=(num_examples, len(w)))\n","    y = nd.dot(X, w) + b\n","    y += nd.random.normal(scale=0.01, shape=y.shape)\n","    return X, y\n","\n","# código para carregar o dataset do MNIST\n","# http://yann.lecun.com/exdb/mnist/\n","def load_data_mnist(batch_size, resize=None, root=os.path.join(\n","        '~', '.mxnet', 'datasets', 'mnist')):\n","    \"\"\"Download the MNIST dataset and then load into memory.\"\"\"\n","    root = os.path.expanduser(root)\n","    transformer = []\n","    if resize:\n","        transformer += [gdata.vision.transforms.Resize(resize)]\n","    transformer += [gdata.vision.transforms.ToTensor()]\n","    transformer = gdata.vision.transforms.Compose(transformer)\n","\n","    mnist_train = gdata.vision.MNIST(root=root, train=True)\n","    mnist_test = gdata.vision.MNIST(root=root, train=False)\n","    num_workers = 0 if sys.platform.startswith('win32') else 4\n","\n","    train_iter = gdata.DataLoader(mnist_train.transform_first(transformer),\n","                                  batch_size, shuffle=True,\n","                                  num_workers=num_workers)\n","    test_iter = gdata.DataLoader(mnist_test.transform_first(transformer),\n","                                 batch_size, shuffle=False,\n","                                 num_workers=num_workers)\n","    return train_iter, test_iter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8oSVf8u1Oi1m","colab_type":"code","colab":{}},"source":["# funções básicas\n","\n","def load_array(features, labels, batch_size, is_train=True):\n","    \"\"\"Construct a Gluon data loader\"\"\"\n","    dataset = gluon.data.ArrayDataset(features, labels)\n","    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n","\n","def _get_batch(batch, ctx):\n","    \"\"\"Return features and labels on ctx.\"\"\"\n","    features, labels = batch\n","    if labels.dtype != features.dtype:\n","        labels = labels.astype(features.dtype)\n","    return (gutils.split_and_load(features, ctx),\n","            gutils.split_and_load(labels, ctx), features.shape[0])\n","\n","# Função usada para calcular acurácia\n","def evaluate_accuracy(data_iter, net, loss, ctx=[mx.cpu()]):\n","    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n","    if isinstance(ctx, mx.Context):\n","        ctx = [ctx]\n","    acc_sum, n, l = nd.array([0]), 0, 0\n","    for batch in data_iter:\n","        features, labels, _ = _get_batch(batch, ctx)\n","        for X, y in zip(features, labels):\n","            # X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n","            y = y.astype('float32')\n","            y_hat = net(X)\n","            l += loss(y_hat, y).sum()\n","            acc_sum += (y_hat.argmax(axis=1) == y).sum().copyto(mx.cpu())\n","            n += y.size\n","        acc_sum.wait_to_read()\n","    return acc_sum.asscalar() / n, l.asscalar() / n\n","  \n","# Função usada no treinamento e validação da rede\n","def train_validate(net, train_iter, test_iter, batch_size, trainer, loss, ctx,\n","                   num_epochs, type='regression'):\n","    print('training on', ctx)\n","    for epoch in range(num_epochs):\n","        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n","        for X, y in train_iter:\n","            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n","            with autograd.record():\n","                y_hat = net(X)\n","                l = loss(y_hat, y).sum()\n","            l.backward()\n","            trainer.step(batch_size)\n","            y = y.astype('float32')\n","            train_l_sum += l.asscalar()\n","            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n","            n += y.size\n","        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss, ctx)\n","        if type == 'regression':\n","          print('epoch %d, train loss %.4f, test loss %.4f, time %.1f sec'\n","                % (epoch + 1, train_l_sum / n, test_loss, time.time() - start))\n","        else:\n","          print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n","                'test acc %.3f, time %.1f sec'\n","                % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_loss, \n","                   test_acc, time.time() - start))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Azv2ajIYkIjH","colab_type":"text"},"source":["## *Loss* L2\n","\n","O função de custo chamada L2 (também conhecida como *Mean Squared Error* -- MSE) é, talvez, a função de perda mais simples e comum. \n","Essa função é representada simplesmente pela média do quadrado da diferença entre as previsões do modelo e o *ground-truth*.\n","Essa função nunca terá valores negativos, pois a diferença calculada será sempre elevado à segunda potência.\n","\n","Formalmente, dado a valor real $y$, e a predição feita pelo modelo $\\hat{y}$, a *loss* L2 é definida pela seguinte equação:\n","\n","$$\\mathcal{l}_2^i(w, b) = \\frac{1}{2} (\\hat{y}^i - y^i)^2 $$\n","\n","A constante $1/2$ é apenas por conveniência matemática, garantindo que depois de tomarmos a derivada dessa função, o coeficiente constante será de $1$.\n","\n","A grande vantagem dessa função é que ela garante que o modelo treinado não tenha previsões discrepantes com erros enormes, já que ela atribui maior peso a esses erros devido à parte quadrática da função.\n","Entretanto, isso gera a desvantagem dessa função de custo, pois se o modelo faz uma única previsão muito ruim, a parte quadrática da função aumenta o erro consideravelmente.\n","No entanto, em muitos casos práticos, não nos importamos muito com esses poucos valores discrepantes e buscamos um modelo mais abrangente que tenha um bom desempenho na maioria.\n","\n","Para tentar garantir a qualidade do modelo em todo o conjunto de dados, podemos simplesmente calcular a média das perdas no conjunto de treinamento:\n","\n","$$\\mathcal{L}(w, b) = \\frac{1}{n} \\sum_i^N \\mathcal{l}^{i}_2(w, b) $$\n","\n","### Implementação\n","\n","Em frameworks atuais (como no MxNet, TensorFlow, e PyTorch), a implementação de funções de custo comuns, como a L2, são diretas e muitos simples.\n","\n","**Um exemplo é mostrado abaixo utilizando o framework MxNet.**"]},{"cell_type":"code","metadata":{"id":"T0LIS1C8S6k1","colab_type":"code","outputId":"e7b2c198-6e96-42a2-cfba-1388955ec82b","executionInfo":{"status":"ok","timestamp":1562159639448,"user_tz":180,"elapsed":900,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":586}},"source":["seed = nd.array([2, -3.4])\n","seed_gt = 4.2\n","features, labels = synthetic_regression_data(seed, seed_gt, 1000)\n","  \n","batch_size = 10\n","data_iter = load_array(features, labels, batch_size)\n","\n","# arquitetura super simples\n","net = nn.Sequential()\n","net.add(nn.Dense(1))\n","net.initialize(init.Normal(sigma=0.01))\n","\n","loss = gloss.L2Loss()  # loss L2\n","trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})\n","\n","# treino\n","num_epochs = 3\n","for epoch in range(1, num_epochs + 1):\n","    for X, y in data_iter:\n","        with autograd.record():\n","            l = loss(net(X), y)\n","        l.backward()\n","        trainer.step(batch_size)\n","    l = loss(net(features), labels)\n","    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n","\n","for i in range(995, 999):\n","  y_hat = net(features[i:i+1, :])\n","  print(y_hat, labels[i], labels[i] - y_hat)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch 1, loss: 0.040588\n","epoch 2, loss: 0.000160\n","epoch 3, loss: 0.000050\n","\n","[[6.0668907]]\n","<NDArray 1x1 @cpu(0)> \n","[6.073719]\n","<NDArray 1 @cpu(0)> \n","[[0.00682831]]\n","<NDArray 1x1 @cpu(0)>\n","\n","[[-0.45210648]]\n","<NDArray 1x1 @cpu(0)> \n","[-0.46103162]\n","<NDArray 1 @cpu(0)> \n","[[-0.00892514]]\n","<NDArray 1x1 @cpu(0)>\n","\n","[[3.819031]]\n","<NDArray 1x1 @cpu(0)> \n","[3.8151767]\n","<NDArray 1 @cpu(0)> \n","[[-0.00385427]]\n","<NDArray 1x1 @cpu(0)>\n","\n","[[8.072802]]\n","<NDArray 1x1 @cpu(0)> \n","[8.061257]\n","<NDArray 1 @cpu(0)> \n","[[-0.01154423]]\n","<NDArray 1x1 @cpu(0)>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lz_ayDVLnNFU","colab_type":"text"},"source":["## *Loss* L1\n","\n","A função de custo L1 é apenas ligeiramente diferente da L2, mas fornece curiosamente propriedades quase exatamente opostas!\n","Essa função é representada pelo valor absoluto da diferença entre as previsões do modelo e o *ground-truth*.\n","\n","Essa função, assim como o *loss* L2, nunca será negativo, pois neste caso estamos sempre assumindo o valor absoluto dos erros.\n","Formalmente, dado a valor real (*ground-truth*) $y$, e a predição feita pelo modelo $\\hat{y}$, a *loss* L1 é definida pela seguinte equação:\n","\n","$$\\mathcal{l}_1^i(w, b) = \\sum_i |\\hat{y}^i - y^i| $$\n","\n","A grande vantagem da função de custo L1 cobre diretamente a desvantagem do *loss* L2.\n","Em outras palaras, como estamos trabalhando com o valor absoluto, todos os erros serão ponderados na mesma escala linear.\n","Assim, ao contrário do *loss* L2, não estamos colocando muito peso nos valores com grande discrepância e a função de perda fornece uma medida genérica e uniforme do desempenho do modelo.\n","\n","Por outro lado, a desvantagem desta função é, para alguns casos, não dar pesos diferentes para específico valores discrepantes. Por exemplo, os erros relativamente grandes provenientes dos *outliers* acabam sendo ponderados exatamente como erros menores. Isso pode resultar em nosso modelo sendo ótimo na maior parte do tempo, mas fazendo algumas previsões muito ruins de vez em quando.\n","\n","Essa função pode ser facilmente implementada no MXNet, como pode ser visto [aqui](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.L1Loss)."]},{"cell_type":"code","metadata":{"id":"gYe6ZR3L9jxc","colab_type":"code","outputId":"4f0a2e72-2e9c-4a6b-993b-adbcd5990e0b","executionInfo":{"status":"ok","timestamp":1562159643552,"user_tz":180,"elapsed":861,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":586}},"source":["seed = nd.array([2, -3.4])\n","seed_gt = 4.2\n","features, labels = synthetic_regression_data(seed, seed_gt, 1000)\n","  \n","batch_size = 10\n","data_iter = load_array(features, labels, batch_size)\n","\n","# arquitetura super simples\n","net = nn.Sequential()\n","net.add(nn.Dense(1))\n","net.initialize(init.Normal(sigma=0.01))\n","\n","loss = gloss.L1Loss()  # loss L1\n","trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})\n","\n","# treino\n","num_epochs = 3\n","for epoch in range(1, num_epochs + 1):\n","    for X, y in data_iter:\n","        with autograd.record():\n","            l = loss(net(X), y)\n","        l.backward()\n","        trainer.step(batch_size)\n","    l = loss(net(features), labels)\n","    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n","\n","for i in range(995, 999):\n","  y_hat = net(features[i:i+1, :])\n","  print(y_hat, labels[i], labels[i] - y_hat)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch 1, loss: 2.686278\n","epoch 2, loss: 0.667559\n","epoch 3, loss: 0.009290\n","\n","[[6.3623056]]\n","<NDArray 1x1 @cpu(0)> \n","[6.3656797]\n","<NDArray 1 @cpu(0)> \n","[[0.0033741]]\n","<NDArray 1x1 @cpu(0)>\n","\n","[[7.00144]]\n","<NDArray 1x1 @cpu(0)> \n","[7.006044]\n","<NDArray 1 @cpu(0)> \n","[[0.00460386]]\n","<NDArray 1x1 @cpu(0)>\n","\n","[[0.8502693]]\n","<NDArray 1x1 @cpu(0)> \n","[0.8532924]\n","<NDArray 1 @cpu(0)> \n","[[0.00302309]]\n","<NDArray 1x1 @cpu(0)>\n","\n","[[3.6928315]]\n","<NDArray 1x1 @cpu(0)> \n","[3.6847017]\n","<NDArray 1 @cpu(0)> \n","[[-0.00812984]]\n","<NDArray 1x1 @cpu(0)>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FfLtl5pE-TYJ","colab_type":"text"},"source":["## Huber *Loss* \n","\n","Vimos que a função de perda L2 tem certas vantangens (como conseguir aprender *outliers*), enquanto o *loss* L1 tem outros benefícios, como ignorar os *outliers*.\n","Porém, existe uma forma de combinar e agregar os benefícios das duas?\n","\n","Sim! A Huber *Loss* oferece o melhor dos dois mundos, equilibrando as funções de perda L1 e L2 juntos. \n","Formalmente, dado a valor real (*ground-truth*) $y$, e a predição feita pelo modelo $\\hat{y}$, a Huber *Loss* é definida pela seguinte equação:\n","\n","$$\n","l_H^i(w, b) = \\sum_i \\begin{cases}\n","                                        \\frac{1}{2\\rho} (\\hat{y}^i - y^i)^2, & \\text{if } |\\hat{y}^i - y^i| < \\rho\\\\\n","                                        |\\hat{y}^i - y^i| - \\frac{\\rho}{2},  & \\text{otherwise}\n","\\end{cases}\n","$$\n",", onde $\\rho$ é uma constance que define a margem.\n","O que essa equação essencialmente diz é: para valores de perda menores que $\\rho$, use o *loss& L2; para valores de perda maiores que delta, use a função de custo L1.\n","Isso efetivamente combina o melhor dos dois mundos das duas funções de perda!\n","\n","O uso da função de custo L1 para valores maiores reduz o peso que colocamos em valores discrepantes para que possamos obter um modelo completo. Ao mesmo tempo, usamos o *loss* L2 para valores menores de perda para manter uma função quadrática próxima ao centro.\n","\n","Essa função pode ser facilmente implementada no MXNet, como pode ser visto [aqui](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.HuberLoss)."]},{"cell_type":"code","metadata":{"id":"98gc8lje-MCL","colab_type":"code","outputId":"267831aa-e6e4-460b-e8ef-1700fb70f62e","executionInfo":{"status":"ok","timestamp":1562159671158,"user_tz":180,"elapsed":1164,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":586}},"source":["seed = nd.array([2, -3.4])\n","seed_gt = 4.2\n","features, labels = synthetic_regression_data(seed, seed_gt, 1000)\n","  \n","batch_size = 10\n","data_iter = load_array(features, labels, batch_size)\n","\n","# arquitetura super simples\n","net = nn.Sequential()\n","net.add(nn.Dense(1))\n","net.initialize(init.Normal(sigma=0.01))\n","\n","loss = gloss.HuberLoss()  # loss Huber\n","trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})\n","\n","# treino\n","num_epochs = 3\n","for epoch in range(1, num_epochs + 1):\n","    for X, y in data_iter:\n","        with autograd.record():\n","            l = loss(net(X), y)\n","        l.backward()\n","        trainer.step(batch_size)\n","    l = loss(net(features), labels)\n","    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))\n","\n","for i in range(995, 999):\n","  y_hat = net(features[i:i+1, :])\n","  print(y_hat, labels[i], labels[i] - y_hat)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch 1, loss: 2.245753\n","epoch 2, loss: 0.361979\n","epoch 3, loss: 0.000923\n","\n","[[0.10509491]]\n","<NDArray 1x1 @cpu(0)> \n","[0.10078488]\n","<NDArray 1 @cpu(0)> \n","[[-0.00431003]]\n","<NDArray 1x1 @cpu(0)>\n","\n","[[5.689843]]\n","<NDArray 1x1 @cpu(0)> \n","[5.727647]\n","<NDArray 1 @cpu(0)> \n","[[0.03780365]]\n","<NDArray 1x1 @cpu(0)>\n","\n","[[5.5963483]]\n","<NDArray 1x1 @cpu(0)> \n","[5.641131]\n","<NDArray 1 @cpu(0)> \n","[[0.04478264]]\n","<NDArray 1x1 @cpu(0)>\n","\n","[[4.2930875]]\n","<NDArray 1x1 @cpu(0)> \n","[4.3207493]\n","<NDArray 1 @cpu(0)> \n","[[0.0276618]]\n","<NDArray 1x1 @cpu(0)>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8fG_HV8k1YOe","colab_type":"text"},"source":["## *Loss Cross-Entropy*\n","\n","O função de custo chamada *cross-entropy* ou *log loss* é a mais usada em problemas de classificação.\n","Essa função de perda, embasada pela teoria da informação, procura penalizar o *loss* baseado somente na classe correta de cada amostra.\n","\n","Formalmente, dado a valor real $y$, e a predição feita pelo modelo $\\hat{y}$, a *loss cross-entropy* é definida pela seguinte equação:\n","\n","$$\\mathcal{l}(w, b) = - \\sum_i y_i log~\\hat{y}_i $$\n",", onde $\\hat{y}$ é saída normalizada (via [softmax](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.Symbol.softmax)) da predição da rede.\n","\n","Em particular, no somatório apenas um termo será diferente de zero e esse termo será o $log$ da probabilidade (normalizada via [softmax](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.Symbol.softmax)) atribuída à classe correta. Intuitivamente, isso faz sentido porque $log (x)$ está aumentando no intervalo (0,1), então $−log (x)$ está diminuindo naquele intervalo.\n","Por exemplo, se tivermos uma amostra com probabilidade de 0.8 para o rótulo correto, o *loos* será penalizado em apenas 0.09.\n","Já se tivermos uma probabilidade menor de 0.08, o *loss* será penalizado em 1,09.\n","\n","### Implementação\n","\n","Em frameworks atuais (como no MxNet, TensorFlow, e PyTorch), a implementação da função de custo *cross-entropy* é direta.\n","\n","**Um exemplo é mostrado abaixo utilizando o framework MxNet.**"]},{"cell_type":"code","metadata":{"id":"EFzoRhT51mW_","colab_type":"code","outputId":"c8085739-bd15-4cb7-9e64-d120191b411b","executionInfo":{"status":"ok","timestamp":1560716058740,"user_tz":180,"elapsed":87421,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":496}},"source":["# parâmetros: número de epochs, learning rate (ou taxa de aprendizado), e \n","# tamanho do batch\n","num_epochs, lr, batch_size = 20, 0.5, 256\n","\n","# rede simples somente com perceptrons e camadas densamente conectadas\n","net = nn.Sequential()\n","net.add(nn.Dense(256, activation=\"relu\"),\n","        nn.Dense(128, activation=\"relu\"),\n","        nn.Dense(64, activation=\"relu\"),\n","        nn.Dense(10))\n","net.initialize(init.Normal(sigma=0.01), ctx=ctx)\n","\n","# função de custo (ou loss)\n","loss = gloss.SoftmaxCrossEntropyLoss()\n","\n","# carregamento do dado: mnist\n","train_iter, test_iter = load_data_mnist(batch_size)\n","\n","# trainer do gluon\n","trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n","\n","# treinamento e validação via MXNet\n","train_validate(net, train_iter, test_iter, batch_size, trainer, loss, \n","               ctx, num_epochs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading /root/.mxnet/datasets/mnist/train-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-images-idx3-ubyte.gz...\n","Downloading /root/.mxnet/datasets/mnist/train-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-labels-idx1-ubyte.gz...\n","Downloading /root/.mxnet/datasets/mnist/t10k-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-images-idx3-ubyte.gz...\n","Downloading /root/.mxnet/datasets/mnist/t10k-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-labels-idx1-ubyte.gz...\n","training on gpu(0)\n","epoch 1, train loss 2.3016, test loss 2.3006, time 3.5 sec\n","epoch 2, train loss 1.9082, test loss 1.1132, time 4.1 sec\n","epoch 3, train loss 0.6266, test loss 0.2657, time 4.1 sec\n","epoch 4, train loss 0.2040, test loss 0.1611, time 4.1 sec\n","epoch 5, train loss 0.1280, test loss 0.1281, time 4.0 sec\n","epoch 6, train loss 0.0950, test loss 0.1042, time 4.1 sec\n","epoch 7, train loss 0.0709, test loss 0.0979, time 4.4 sec\n","epoch 8, train loss 0.0577, test loss 0.0889, time 4.1 sec\n","epoch 9, train loss 0.0457, test loss 0.0887, time 4.1 sec\n","epoch 10, train loss 0.0401, test loss 0.0860, time 4.2 sec\n","epoch 11, train loss 0.0318, test loss 0.0812, time 4.0 sec\n","epoch 12, train loss 0.0265, test loss 0.0869, time 4.1 sec\n","epoch 13, train loss 0.0193, test loss 0.1054, time 4.2 sec\n","epoch 14, train loss 0.0178, test loss 0.0950, time 4.1 sec\n","epoch 15, train loss 0.0128, test loss 0.0893, time 4.3 sec\n","epoch 16, train loss 0.0086, test loss 0.1041, time 4.2 sec\n","epoch 17, train loss 0.0063, test loss 0.1071, time 4.2 sec\n","epoch 18, train loss 0.0047, test loss 0.0982, time 4.3 sec\n","epoch 19, train loss 0.0027, test loss 0.0988, time 4.3 sec\n","epoch 20, train loss 0.0015, test loss 0.0971, time 4.1 sec\n"],"name":"stdout"}]}]}