{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04.1 - Problemas.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["Y0m-qic-0Wnl","Pd8hG7HCDUib","IDaRVNq1aMpm"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ed03SC1Jm9Yy"},"source":["# Problemas\n","\n","Como vimos acima, há muitos passos na criação e definição de uma nova rede neural.\n","A grande parte desses ajustes dependem diretamente do problemas.\n","\n","Abaixo, listamos alguns problemas. Todos os problemas e datasets usados vem do [Center for Machine Learning and Intelligent Systems](http://archive.ics.uci.edu/ml/datasets.php).\n","\n","\n","**Seu objetivo é determinar e implementar um modelo para cada problema.**\n","\n","Isso inclui definir uma arquitetura (por enquanto usando somente camadas [Densas](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.Dense), porém podemos variar as ativações -- [Sigmoid](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.Symbol.sigmoid), [Tanh](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.Symbol.tanh), [ReLU](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.Symbol.relu), [LeakyReLU, ELU, SeLU, PReLU, RReLU](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.LeakyReLU)), uma função de custo ( [L1](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.L2Loss), [L2](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.L1Loss),[ Huber](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.HuberLoss), [*Cross-Entropy*](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss), [Hinge](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.HingeLoss)), e um algoritmo de otimização ([SGD](https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.SGD), [Momentum](https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.SGD), [RMSProp](https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.RMSProp), [Adam](https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.Adam)).\n","\n","A leitura do dado assim como a função de treinamento já estão implementados."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Gp6CwWnFnTwb"},"source":["Esse pequeno bloco de código abaixo é usado somente para instalar o MXNet para CUDA 10. Execute esse bloco somente uma vez e ignore possíveis erros levantados durante a instalação.\n","\n","**ATENÇÃO: a alteração deste bloco pode implicar em problemas na execução dos blocos restantes!**"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":50053,"status":"ok","timestamp":1560719499043,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"},"user_tz":180},"id":"XW-VATPAldgt","outputId":"34cb52c1-e8d4-4d9f-c18d-62d96bd86d9b","colab":{"base_uri":"https://localhost:8080/","height":643}},"source":["!pip install mxnet-cu100\n","\n","# imports basicos\n","import time, os, sys, numpy as np\n","import mxnet as mx\n","from mxnet import autograd, gluon, init, nd\n","from mxnet.gluon import loss as gloss, nn, utils as gutils, data as gdata\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","\n","# Tenta encontrar GPU\n","def try_gpu():\n","    try:\n","        ctx = mx.gpu()\n","        _ = nd.zeros((1,), ctx=ctx)\n","    except mx.base.MXNetError:\n","        ctx = mx.cpu()\n","    return ctx\n","\n","ctx = try_gpu()\n","ctx"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting mxnet-cu100\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/91/b5c2692297aa5b8c383e0da18f9208fc6d5519d981c03266abfbde897c41/mxnet_cu100-1.4.1-py2.py3-none-manylinux1_x86_64.whl (488.3MB)\n","\u001b[K     |████████████████████████████████| 488.3MB 27kB/s \n","\u001b[?25hCollecting numpy<1.15.0,>=1.8.2 (from mxnet-cu100)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c4/395ebb218053ba44d64935b3729bc88241ec279915e72100c5979db10945/numpy-1.14.6-cp36-cp36m-manylinux1_x86_64.whl (13.8MB)\n","\u001b[K     |████████████████████████████████| 13.8MB 34.3MB/s \n","\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1 (from mxnet-cu100)\n","  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n","Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (2.21.0)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2019.3.9)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (1.24.3)\n","\u001b[31mERROR: spacy 2.1.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fastai 1.0.52 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: blis 0.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, graphviz, mxnet-cu100\n","  Found existing installation: numpy 1.16.4\n","    Uninstalling numpy-1.16.4:\n","      Successfully uninstalled numpy-1.16.4\n","  Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.8.4 mxnet-cu100-1.4.1 numpy-1.14.6\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["gpu(0)"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8oSVf8u1Oi1m","colab":{}},"source":["# funções básicas\n","\n","def load_array(features, labels, batch_size, is_train=True):\n","    \"\"\"Construct a Gluon data loader\"\"\"\n","    dataset = gluon.data.ArrayDataset(features, labels)\n","    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n","\n","def _get_batch(batch, ctx):\n","    \"\"\"Return features and labels on ctx.\"\"\"\n","    features, labels = batch\n","    if labels.dtype != features.dtype:\n","        labels = labels.astype(features.dtype)\n","    return (gutils.split_and_load(features, ctx),\n","            gutils.split_and_load(labels, ctx), features.shape[0])\n","\n","# Função usada para calcular acurácia\n","def evaluate_accuracy(data_iter, net, loss, ctx=[mx.cpu()]):\n","    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n","    if isinstance(ctx, mx.Context):\n","        ctx = [ctx]\n","    acc_sum, n, l = nd.array([0]), 0, 0\n","    for batch in data_iter:\n","        features, labels, _ = _get_batch(batch, ctx)\n","        for X, y in zip(features, labels):\n","            # X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n","            y = y.astype('float32')\n","            y_hat = net(X)\n","            l += loss(y_hat, y).sum()\n","            acc_sum += (y_hat.argmax(axis=1) == y).sum().copyto(mx.cpu())\n","            n += y.size\n","        acc_sum.wait_to_read()\n","    return acc_sum.asscalar() / n, l.asscalar() / n\n","  \n","# Função usada no treinamento e validação da rede\n","def train_validate(net, train_iter, test_iter, batch_size, trainer, loss, ctx,\n","                   num_epochs, type='regression'):\n","    print('training on', ctx)\n","    for epoch in range(num_epochs):\n","        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n","        for X, y in train_iter:\n","            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n","            with autograd.record():\n","                y_hat = net(X)\n","                l = loss(y_hat, y).sum()\n","            l.backward()\n","            trainer.step(batch_size)\n","            y = y.astype('float32')\n","            train_l_sum += l.asscalar()\n","            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n","            n += y.size\n","        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss, ctx)\n","        if type == 'regression':\n","          print('epoch %d, train loss %.4f, test loss %.4f, time %.1f sec'\n","                % (epoch + 1, train_l_sum / n, test_loss, time.time() - start))\n","        else:\n","          print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n","                'test acc %.3f, time %.1f sec'\n","                % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_loss, \n","                   test_acc, time.time() - start))\n","          \n","# funcao usada para teste\n","def test(net, test_iter):\n","    print('testing on', ctx)\n","    first = True\n","    for X in test_iter:\n","        X = X.as_in_context(ctx)\n","        y_hat = net(X)\n","        if first is True:\n","          pred_logits = y_hat\n","          pred_labels = y_hat.argmax(axis=1)\n","          first = False\n","        else:\n","          pred_logits = nd.concat(pred_logits, y_hat, dim=0)\n","          pred_labels = nd.concat(pred_labels, y_hat.argmax(axis=1), dim=0)\n","\n","    return pred_logits.asnumpy(), pred_labels.asnumpy()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y0m-qic-0Wnl","colab_type":"text"},"source":["## Problema 1\n","\n","Neste problema, você receberá 7 *features* extraídas de poços de petróleo ('BRCALI', 'BRDENS', 'BRDTP', 'BRGR', 'BRNEUT', 'BRRESC', 'BRRESP') e deve predizer o tipo de rocha."]},{"cell_type":"markdown","metadata":{"id":"U64ACnJoGsDv","colab_type":"text"},"source":["### Treino e Validação\n","\n","Primeiro, vamos modelar uma rede neural e treiná-la.\n","Usamos o dado de treino carregado no próximo bloco para convergir o modelo e o dado de validação para avaliar quão bom ele estão. "]},{"cell_type":"code","metadata":{"id":"AUYOPZYH0Ztc","colab_type":"code","colab":{}},"source":["# download do dataset\n","!wget https://www.dropbox.com/s/ujnqxh6l43tlbdi/poco_1.prn\n","X = np.loadtxt('poco_1.prn', skiprows=11, usecols=(1,2,3,4,5,6,7), dtype=np.float32)\n","y = np.loadtxt('poco_1.prn', skiprows=11, usecols=8, dtype=np.str)\n","print(y)\n","print(set(y))\n","le = preprocessing.LabelEncoder()\n","le.fit(list(set(y)))\n","y_t = le.transform(y)\n","\n","print(X[0, :])\n","print(y[0], y_t[0])\n","print(y[960], y_t[960])\n","train_features, test_features, train_labels, test_labels = train_test_split(X, y_t, test_size=0.33, random_state=42)\n","\n","def load_array(features, labels, batch_size, is_train=True):\n","    \"\"\"Construct a Gluon data loader\"\"\"\n","    dataset = gluon.data.ArrayDataset(features, labels)\n","    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n","  \n","batch_size = 100\n","train_iter = load_array(train_features, train_labels, batch_size)\n","test_iter = load_array(test_features, test_labels, batch_size, False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AJrn2iOrbEWy","colab_type":"text"},"source":["### Teste\n","\n","Agora, com a rede treinada, a usaremos para predizer as classes de novas amostras de teste (cujo qual não temos as classes)."]},{"cell_type":"code","metadata":{"id":"rpjqbJEQbFce","colab_type":"code","colab":{}},"source":["# download do dataset\n","!wget https://www.dropbox.com/s/vsduomvr10eg0wz/poco_2.prn\n","test_data = np.loadtxt('poco_2.prn', skiprows=11, usecols=(1,2,3,5,6,7,8), dtype=np.float32)\n","print(test_data.shape)\n","\n","def load_array_test(features, batch_size, is_train=True):\n","    \"\"\"Construct a Gluon data loader\"\"\"\n","    dataset = gluon.data.ArrayDataset(features)\n","    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n","  \n","batch_size = 100\n","test_iter = load_array_test(test_data, batch_size, False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Pd8hG7HCDUib"},"source":["## Problema 2\n","\n","Neste problema, você receberá várias *features* (como altura média, inclinação, etc) descrevendo uma região e o modelo deve predizer qual o tipo da região (floresta, montanha, etc)."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":32206,"status":"ok","timestamp":1560720870949,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"},"user_tz":180},"id":"IZcIXGqBDznB","outputId":"c74113e3-44ed-49b4-a6fd-2d1b9a5e5315","colab":{"base_uri":"https://localhost:8080/","height":550}},"source":["!wget http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n","!gzip covtype.data.gz\n","data = np.genfromtxt('covtype.data', delimiter=',', dtype=np.float32)\n","\n","print(data.shape, data[0, :])\n","X, y = data[:, :-1], data[:, -1]\n","print(X.shape, X[0, :])\n","print(y.shape, y[0]k)\n","train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","def load_array(features, labels, batch_size, is_train=True):\n","    \"\"\"Construct a Gluon data loader\"\"\"\n","    dataset = gluon.data.ArrayDataset(features, labels)\n","    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n","  \n","batch_size = 100\n","train_iter = load_array(train_features, train_labels, batch_size)\n","test_iter = load_array(test_features, test_labels, batch_size, False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-06-16 21:33:59--  http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n","Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n","Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11240707 (11M) [application/x-httpd-php]\n","Saving to: ‘covtype.data.gz.2’\n","\n","covtype.data.gz.2   100%[===================>]  10.72M  13.0MB/s    in 0.8s    \n","\n","2019-06-16 21:34:00 (13.0 MB/s) - ‘covtype.data.gz.2’ saved [11240707/11240707]\n","\n","gzip: covtype.data.gz already has .gz suffix -- unchanged\n","(581012, 55) [2.596e+03 5.100e+01 3.000e+00 2.580e+02 0.000e+00 5.100e+02 2.210e+02\n"," 2.320e+02 1.480e+02 6.279e+03 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n"," 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n"," 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n"," 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n"," 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n"," 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n"," 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00]\n","(581012, 54) [2.596e+03 5.100e+01 3.000e+00 2.580e+02 0.000e+00 5.100e+02 2.210e+02\n"," 2.320e+02 1.480e+02 6.279e+03 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n"," 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n"," 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n"," 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n"," 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n"," 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n"," 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n","(581012,) 5.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IDaRVNq1aMpm"},"source":["## Problema 3\n","\n","Neste problema, você receberá 90 *features* extraídas de diversas músicas (datadas de 1922 até 2011) e deve predizer o ano de cada música."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":71363,"status":"ok","timestamp":1560719574482,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"},"user_tz":180},"id":"CWdBT3zhW_Y5","outputId":"bb58ec97-b181-4fd8-9533-de227fd124bb","colab":{"base_uri":"https://localhost:8080/","height":678}},"source":["# download do dataset\n","!wget http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n","!unzip YearPredictionMSD.txt.zip\n","data = np.genfromtxt('YearPredictionMSD.txt', delimiter=',', dtype=np.float32)\n","\n","print(data[0, :])\n","X, y = data[:, 1:], data[:, 0]\n","train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","def load_array(features, labels, batch_size, is_train=True):\n","    \"\"\"Construct a Gluon data loader\"\"\"\n","    dataset = gluon.data.ArrayDataset(features, labels)\n","    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n","  \n","batch_size = 100\n","train_iter = load_array(train_features, train_labels, batch_size)\n","test_iter = load_array(test_features, test_labels, batch_size, False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-06-16 21:11:44--  http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n","Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n","Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 211011981 (201M) [application/x-httpd-php]\n","Saving to: ‘YearPredictionMSD.txt.zip’\n","\n","YearPredictionMSD.t 100%[===================>] 201.24M  43.4MB/s    in 5.1s    \n","\n","2019-06-16 21:11:49 (39.4 MB/s) - ‘YearPredictionMSD.txt.zip’ saved [211011981/211011981]\n","\n","Archive:  YearPredictionMSD.txt.zip\n","  inflating: YearPredictionMSD.txt   \n","[ 2.0010000e+03  4.9943569e+01  2.1471140e+01  7.3077499e+01\n","  8.7486095e+00 -1.7406281e+01 -1.3099050e+01 -2.5012020e+01\n"," -1.2232570e+01  7.8308902e+00 -2.4678299e+00  3.3213601e+00\n"," -2.3152101e+00  1.0205560e+01  6.1110913e+02  9.5108960e+02\n","  6.9811426e+02  4.0898486e+02  3.8370911e+02  3.2651511e+02\n","  2.3811327e+02  2.5142413e+02  1.8717351e+02  1.0042652e+02\n","  1.7919498e+02 -8.4155798e+00 -3.1787039e+02  9.5862663e+01\n","  4.8102589e+01 -9.5663033e+01 -1.8062149e+01  1.9698400e+00\n","  3.4424381e+01  1.1726700e+01  1.3679000e+00  7.7944398e+00\n"," -3.6994001e-01 -1.3367851e+02 -8.3261650e+01 -3.7297649e+01\n","  7.3046669e+01 -3.7366840e+01 -3.1385300e+00 -2.4215309e+01\n"," -1.3230660e+01  1.5938090e+01 -1.8604780e+01  8.2154793e+01\n","  2.4057980e+02 -1.0294070e+01  3.1584311e+01 -2.5381870e+01\n"," -3.9077201e+00  1.3292580e+01  4.1550598e+01 -7.2627201e+00\n"," -2.1008631e+01  1.0550848e+02  6.4298561e+01  2.6084810e+01\n"," -4.4591099e+01 -8.3065701e+00  7.9370599e+00 -1.0736600e+01\n"," -9.5447662e+01 -8.2033073e+01 -3.5591942e+01  4.6952500e+00\n","  7.0956261e+01  2.8091391e+01  6.0201502e+00 -3.7137669e+01\n"," -4.1124500e+01 -8.4081602e+00  7.1987700e+00 -8.6017599e+00\n"," -5.9085698e+00 -1.2324370e+01  1.4687340e+01 -5.4321251e+01\n","  4.0147861e+01  1.3016200e+01 -5.4405479e+01  5.8993671e+01\n","  1.5373440e+01  1.1114399e+00 -2.3087931e+01  6.8407951e+01\n"," -1.8222300e+00 -2.7463480e+01  2.2632699e+00]\n"],"name":"stdout"}]}]}