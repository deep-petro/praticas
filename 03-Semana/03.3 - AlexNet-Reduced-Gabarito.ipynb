{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03.3 - AlexNet-Reduced-Gabarito.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ed03SC1Jm9Yy","colab_type":"text"},"source":["# Prática: AlexNet com recursos limitados\n","\n","Se antes implementamos a [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) sem considerar a questão de quantidade de parâmetros, nesta prática focaremos nesse quesito.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Dx_n6rukq1RG","colab_type":"code","outputId":"f441a1e4-3fc1-4e9c-a1c1-4c2ba7d76df0","executionInfo":{"status":"ok","timestamp":1562764721816,"user_tz":180,"elapsed":7423,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":164}},"source":["!pip install mxnet-cu100\n","\n","# imports basicos\n","import time, os, sys, numpy as np\n","import mxnet as mx\n","from mxnet import autograd, gluon, init, nd\n","from mxnet.gluon import loss as gloss, nn, utils as gutils, data as gdata\n","\n","# Tenta encontrar GPU\n","def try_gpu():\n","    try:\n","        ctx = mx.gpu()\n","        _ = nd.zeros((1,), ctx=ctx)\n","    except mx.base.MXNetError:\n","        ctx = mx.cpu()\n","    return ctx\n","\n","ctx = try_gpu()\n","ctx\n","\n","## carregando dados\n","\n","# código para carregar o dataset do CIFAR 10\n","# https://www.cs.toronto.edu/~kriz/cifar.html\n","def load_data_cifar10(batch_size, resize=None, root=os.path.join(\n","        '~', '.mxnet', 'datasets', 'cifar10')):\n","    \"\"\"Download the MNIST dataset and then load into memory.\"\"\"\n","    root = os.path.expanduser(root)\n","    transformer = []\n","    if resize:\n","        transformer += [gdata.vision.transforms.Resize(resize)]\n","    transformer += [gdata.vision.transforms.ToTensor()]\n","    transformer = gdata.vision.transforms.Compose(transformer)\n","\n","    mnist_train = gdata.vision.CIFAR10(root=root, train=True)\n","    mnist_test = gdata.vision.CIFAR10(root=root, train=False)\n","    num_workers = 0 if sys.platform.startswith('win32') else 4\n","\n","    train_iter = gdata.DataLoader(mnist_train.transform_first(transformer),\n","                                  batch_size, shuffle=True,\n","                                  num_workers=num_workers)\n","    test_iter = gdata.DataLoader(mnist_test.transform_first(transformer),\n","                                 batch_size, shuffle=False,\n","                                 num_workers=num_workers)\n","    return train_iter, test_iter\n","  \n","# funções básicas\n","def _get_batch(batch, ctx):\n","    \"\"\"Return features and labels on ctx.\"\"\"\n","    features, labels = batch\n","    if labels.dtype != features.dtype:\n","        labels = labels.astype(features.dtype)\n","    return (gutils.split_and_load(features, ctx),\n","            gutils.split_and_load(labels, ctx), features.shape[0])\n","\n","# Função usada para calcular acurácia\n","def evaluate_accuracy(data_iter, net, loss, ctx=[mx.cpu()]):\n","    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n","    if isinstance(ctx, mx.Context):\n","        ctx = [ctx]\n","    acc_sum, n, l = nd.array([0]), 0, 0\n","    for batch in data_iter:\n","        features, labels, _ = _get_batch(batch, ctx)\n","        for X, y in zip(features, labels):\n","            # X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n","            y = y.astype('float32')\n","            y_hat = net(X)\n","            l += loss(y_hat, y).sum()\n","            acc_sum += (y_hat.argmax(axis=1) == y).sum().copyto(mx.cpu())\n","            n += y.size\n","        acc_sum.wait_to_read()\n","    return acc_sum.asscalar() / n, l.asscalar() / n\n","  \n","# Função usada no treinamento e validação da rede\n","def train_validate(net, train_iter, test_iter, batch_size, trainer, loss, ctx,\n","                   num_epochs):\n","    print('training on', ctx)\n","    for epoch in range(num_epochs):\n","        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n","        for X, y in train_iter:\n","            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n","            with autograd.record():\n","                y_hat = net(X)\n","                l = loss(y_hat, y).sum()\n","            l.backward()\n","            trainer.step(batch_size)\n","            y = y.astype('float32')\n","            train_l_sum += l.asscalar()\n","            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n","            n += y.size\n","        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss, ctx)\n","        print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n","              'test acc %.3f, time %.1f sec'\n","              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_loss, \n","                 test_acc, time.time() - start))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: mxnet-cu100 in /usr/local/lib/python3.6/dist-packages (1.4.1)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (0.8.4)\n","Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (2.21.0)\n","Requirement already satisfied: numpy<1.15.0,>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (1.14.6)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2019.6.16)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet-cu100) (3.0.4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mA3x5yGAiRDG","colab_type":"text"},"source":["## AlexNet\n","\n","Como vimos, a rede [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) foi uma das arquiteturas mais famosas dessa nova onda de rede neurais.\n","\n","<p align=\"center\">\n","  <img width=700 src=\"https://engmrk.com/wp-content/uploads/2018/10/AlexNet_Summary_Table.jpg\">\n","</p>\n","\n","Entretanto, ela possui muitos parâmetros.\n","Especificamente, essa arquitetura, para classificar 10 classes, tem um total de **58.312.736** de parâmetros como mostrado na tabela abaixo.\n","\n","**Camada** | **Calc Parâmetros** | **Total Parâmetros**\n","--- | ---: | ---:\n","Convolução 1 | 11\\*11\\*3\\*96 | 34.848\n","Convolução 2 | 5\\*5\\*96\\*256 | 614.400\n","Convolução 3 | 3\\*3\\*256\\*384 | 884.736\n","Convolução 4 | 3\\*3\\*384\\*384 | 1.327.104\n","Convolução 5 | 3\\*3\\*384\\*256 | 884.736\n","FC 6 | 9216*4096 | 37.748.736\n","FC 7 | 4096*4096 | 16.777.216\n","FC 8 | 4096*10 | 40.960\n","**Total** | | **58.312.736**\n","\n","**Seu objetivo nessa prática é propor uma nova rede neural, baseada na [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), que possuia MENOS parâmetros e alcance uma acurácia similar ou melhor que a rede original vista na aula passada.**\n","\n","Procure usar [*batch normalization*](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.BatchNorm), camadas dilatadas e separáveis.\n","Neste caso, desconsidere os parâmetros da camada [*batch normalization*](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.BatchNorm)."]},{"cell_type":"markdown","metadata":{"id":"UDMoLdloZXqo","colab_type":"text"},"source":["### Arquitetura 1\n","\n","Essa primeira versão da [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) usa [*batch normalization*](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.BatchNorm) para melhorar o resultado. Esse arquitetura é somente para efeitos de comparação já que, por enquanto, ainda temos a mesma quantidade de parâmetros da versão original."]},{"cell_type":"code","metadata":{"id":"JfmuQQZnLqzS","colab_type":"code","colab":{}},"source":["class AlexNet(nn.HybridBlock):\n","    r\"\"\"AlexNet model from the `\"One weird trick...\" `_ paper.\n","\n","    Parameters\n","    ----------\n","    classes : int, default 10\n","        Number of classes for the output layer.\n","    \"\"\"\n","    def __init__(self, classes=10, **kwargs):\n","        super(AlexNet, self).__init__(**kwargs)\n","        with self.name_scope():\n","            self.features = nn.HybridSequential(prefix='')\n","            with self.features.name_scope():\n","                self.features.add(nn.Conv2D(96, kernel_size=11, strides=4, padding=0))                     # entrada: (b, 3, 227, 227) e saida: (b, 96, 55, 55)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.MaxPool2D(pool_size=3, strides=2))                                    # entrada: (b, 96, 55, 55) e saida: (b, 96, 27, 27)\n","                self.features.add(nn.Conv2D(256, kernel_size=5, padding=2))                                # entrada: (b, 96, 27, 27) e saida: (b, 256, 27, 27)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.MaxPool2D(pool_size=3, strides=2))                                    # entrada: (b, 256, 27, 27) e saida: (b, 256, 13, 13)\n","                self.features.add(nn.Conv2D(384, kernel_size=3, padding=1))                                # entrada: (b, 256, 13, 13) e saida: (b, 384, 13, 13)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.Conv2D(384, kernel_size=3, padding=1))                                # entrada: (b, 384, 13, 13) e saida: (b, 384, 13, 13)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.Conv2D(256, kernel_size=3, padding=1))                                # entrada: (b, 384, 13, 13) e saida: (b, 256, 13, 13)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.MaxPool2D(pool_size=3, strides=2))                                    # entrada: (b, 256, 13, 13) e saida: (b, 256, 6, 6)\n","                self.features.add(nn.Flatten())                                                            # entrada: (b, 256, 13, 13) e saida: (b, 256*6*6) = (b, 9216)\n","                self.features.add(nn.Dense(4096))                                                          # entrada: (b, 9216) e saida: (b, 4096)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.Dropout(0.5))\n","                self.features.add(nn.Dense(4096))                                                           # entrada: (b, 4096) e saida: (b, 4096)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.Dropout(0.5))\n","\n","            self.output = nn.Dense(classes)  # entrada: (b, 4096) e saida: (b, 10)\n","\n","    def hybrid_forward(self, F, x):\n","        x = self.features(x)\n","        x = self.output(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5XMMywrQLs3w","colab_type":"code","outputId":"f091c104-f928-4bc0-c374-e065b91eebba","executionInfo":{"status":"ok","timestamp":1562722695210,"user_tz":180,"elapsed":1780240,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":403}},"source":["num_epochs, lr, batch_size, wd_lambda = 20, 0.001, 100, 0.0001\n","    \n","net = AlexNet()\n","net.initialize(init.Normal(sigma=0.01), ctx=ctx)\n","\n","# função de custo (ou loss)\n","loss = gloss.SoftmaxCrossEntropyLoss()\n","\n","# carregamento do dado: fashion mnist\n","train_iter, test_iter = load_data_cifar10(batch_size, resize=227)\n","\n","# trainer do gluon\n","trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr, 'wd': wd_lambda})\n","\n","# treinamento e validação via MXNet\n","train_validate(net, train_iter, test_iter, batch_size, trainer, loss, \n","               ctx, num_epochs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["training on gpu(0)\n","epoch 1, train loss 1.4996, train acc 0.489, test loss 1.4287, test acc 0.520, time 140.6 sec\n","epoch 2, train loss 1.0128, train acc 0.654, test loss 0.9690, test acc 0.666, time 112.8 sec\n","epoch 3, train loss 0.8140, train acc 0.720, test loss 0.9064, test acc 0.692, time 85.1 sec\n","epoch 4, train loss 0.7048, train acc 0.758, test loss 0.7015, test acc 0.761, time 84.9 sec\n","epoch 5, train loss 0.6302, train acc 0.784, test loss 0.7343, test acc 0.751, time 84.9 sec\n","epoch 6, train loss 0.5679, train acc 0.805, test loss 0.7254, test acc 0.758, time 84.9 sec\n","epoch 7, train loss 0.5213, train acc 0.820, test loss 0.7437, test acc 0.751, time 84.8 sec\n","epoch 8, train loss 0.4725, train acc 0.837, test loss 0.6186, test acc 0.796, time 84.9 sec\n","epoch 9, train loss 0.4364, train acc 0.849, test loss 0.5910, test acc 0.801, time 84.8 sec\n","epoch 10, train loss 0.4001, train acc 0.862, test loss 0.7092, test acc 0.772, time 84.6 sec\n","epoch 11, train loss 0.3738, train acc 0.871, test loss 0.6328, test acc 0.796, time 84.6 sec\n","epoch 12, train loss 0.3312, train acc 0.886, test loss 0.5829, test acc 0.814, time 84.5 sec\n","epoch 13, train loss 0.2983, train acc 0.897, test loss 0.5692, test acc 0.811, time 84.6 sec\n","epoch 14, train loss 0.2822, train acc 0.902, test loss 0.5360, test acc 0.829, time 84.6 sec\n","epoch 15, train loss 0.2532, train acc 0.912, test loss 0.5680, test acc 0.827, time 84.4 sec\n","epoch 16, train loss 0.2328, train acc 0.919, test loss 0.6552, test acc 0.799, time 84.4 sec\n","epoch 17, train loss 0.2171, train acc 0.924, test loss 0.6559, test acc 0.804, time 84.3 sec\n","epoch 18, train loss 0.1967, train acc 0.933, test loss 0.6946, test acc 0.790, time 84.6 sec\n","epoch 19, train loss 0.1864, train acc 0.935, test loss 0.6248, test acc 0.818, time 84.3 sec\n","epoch 20, train loss 0.1752, train acc 0.940, test loss 0.5920, test acc 0.829, time 84.4 sec\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nfU8HPvJZup5","colab_type":"text"},"source":["### Arquitetura 2\n","\n","Essa segunda versão da [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) usa [*batch normalization*](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.BatchNorm) e camadas de [convolução dilatada](https://beta.mxnet.io/api/gluon/_autogen/mxnet.gluon.nn.Conv2D.html).\n","Neste caso, como usamos filtros dilatados em duas camadas (com dilatação aumentante, ou seja, dilatação 2 seguida da dilatação 4), foram removidos duas camadas convolucionais, já que o *receptive field* se mantem similar dessa forma.\n","\n","Nessa arquitetura, já temos menos parâmetros. Precisamente, temos \n","\n"]},{"cell_type":"code","metadata":{"id":"Fi4WOPu4i0us","colab_type":"code","colab":{}},"source":["class AlexNet(nn.HybridBlock):\n","    r\"\"\"AlexNet model from the `\"One weird trick...\" `_ paper.\n","\n","    Parameters\n","    ----------\n","    classes : int, default 10\n","        Number of classes for the output layer.\n","    \"\"\"\n","    def __init__(self, classes=10, **kwargs):\n","        super(AlexNet, self).__init__(**kwargs)\n","        with self.name_scope():\n","            self.features = nn.HybridSequential(prefix='')\n","            with self.features.name_scope():\n","                self.features.add(nn.Conv2D(96, kernel_size=11, strides=4, padding=0))                     # entrada: (b, 3, 227, 227) e saida: (b, 96, 55, 55)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.MaxPool2D(pool_size=3, strides=2))                                    # entrada: (b, 96, 55, 55) e saida: (b, 96, 27, 27)\n","                self.features.add(nn.Conv2D(256, kernel_size=5, padding=2, dilation=2))                    # entrada: (b, 96, 27, 27) e saida: (b, 256, 27, 27)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.MaxPool2D(pool_size=3, strides=2))                                    # entrada: (b, 256, 27, 27) e saida: (b, 256, 13, 13)\n","#                 self.features.add(nn.Conv2D(384, kernel_size=3, padding=1))                              # entrada: (b, 256, 13, 13) e saida: (b, 384, 13, 13)\n","#                 self.features.add(nn.BatchNorm())\n","#                 self.features.add(nn.Activation(activation='relu'))\n","#                 self.features.add(nn.Conv2D(384, kernel_size=3, padding=1))                              # entrada: (b, 384, 13, 13) e saida: (b, 384, 13, 13)\n","#                 self.features.add(nn.BatchNorm())\n","#                 self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.Conv2D(256, kernel_size=3, padding=1, dilation=4))                    # entrada: (b, 256, 13, 13) e saida: (b, 256, 13, 13)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.MaxPool2D(pool_size=3, strides=2))                                    # entrada: (b, 256, 13, 13) e saida: (b, 256, 6, 6)\n","                self.features.add(nn.Flatten())                                                            # entrada: (b, 256, 6, 6) e saida: (b, 256*6*6) = (b, 9216)\n","                self.features.add(nn.Dense(4096))                                                          # entrada: (b, 9216) e saida: (b, 4096)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.Dropout(0.5))\n","                self.features.add(nn.Dense(4096))                                                           # entrada: (b, 4096) e saida: (b, 4096)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.Dropout(0.5))\n","\n","            self.output = nn.Dense(classes)  # entrada: (b, 4096) e saida: (b, 10)\n","\n","    def hybrid_forward(self, F, x):\n","        x = self.features(x)\n","        x = self.output(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghmYljpKjIQp","colab_type":"code","outputId":"d2f9b4e4-2052-4edd-a7e2-50d4d4728283","executionInfo":{"status":"ok","timestamp":1562725920558,"user_tz":180,"elapsed":1086421,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"colab":{"base_uri":"https://localhost:8080/","height":403}},"source":["num_epochs, lr, batch_size, wd_lambda = 20, 0.001, 100, 0.0001\n","    \n","net = AlexNet()\n","net.initialize(init.Normal(sigma=0.01), ctx=ctx)\n","\n","# função de custo (ou loss)\n","loss = gloss.SoftmaxCrossEntropyLoss()\n","\n","# carregamento do dado: fashion mnist\n","train_iter, test_iter = load_data_cifar10(batch_size, resize=227)\n","\n","# trainer do gluon\n","trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr, 'wd': wd_lambda})\n","\n","# treinamento e validação via MXNet\n","train_validate(net, train_iter, test_iter, batch_size, trainer, loss, \n","               ctx, num_epochs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["training on gpu(0)\n","epoch 1, train loss 1.4751, train acc 0.489, test loss 1.2504, test acc 0.561, time 78.7 sec\n","epoch 2, train loss 1.0481, train acc 0.637, test loss 0.9986, test acc 0.650, time 77.8 sec\n","epoch 3, train loss 0.8699, train acc 0.700, test loss 1.1546, test acc 0.626, time 77.7 sec\n","epoch 4, train loss 0.7454, train acc 0.745, test loss 0.8249, test acc 0.715, time 77.0 sec\n","epoch 5, train loss 0.6713, train acc 0.768, test loss 0.8899, test acc 0.711, time 76.3 sec\n","epoch 6, train loss 0.5727, train acc 0.800, test loss 0.7583, test acc 0.741, time 76.1 sec\n","epoch 7, train loss 0.5092, train acc 0.821, test loss 1.1919, test acc 0.657, time 76.6 sec\n","epoch 8, train loss 0.4513, train acc 0.842, test loss 1.0681, test acc 0.667, time 75.9 sec\n","epoch 9, train loss 0.3976, train acc 0.861, test loss 0.7757, test acc 0.753, time 75.8 sec\n","epoch 10, train loss 0.3453, train acc 0.880, test loss 0.8760, test acc 0.739, time 75.7 sec\n","epoch 11, train loss 0.3094, train acc 0.892, test loss 0.8422, test acc 0.749, time 76.0 sec\n","epoch 12, train loss 0.2710, train acc 0.907, test loss 0.9534, test acc 0.733, time 75.5 sec\n","epoch 13, train loss 0.2420, train acc 0.915, test loss 0.8340, test acc 0.754, time 75.5 sec\n","epoch 14, train loss 0.2186, train acc 0.925, test loss 0.9228, test acc 0.753, time 75.4 sec\n","epoch 15, train loss 0.2004, train acc 0.931, test loss 0.9309, test acc 0.745, time 75.6 sec\n","epoch 16, train loss 0.1826, train acc 0.937, test loss 0.9265, test acc 0.761, time 75.6 sec\n","epoch 17, train loss 0.1760, train acc 0.939, test loss 1.0700, test acc 0.730, time 75.2 sec\n","epoch 18, train loss 0.1607, train acc 0.944, test loss 0.9324, test acc 0.756, time 75.4 sec\n","epoch 19, train loss 0.1464, train acc 0.949, test loss 0.9428, test acc 0.766, time 75.7 sec\n","epoch 20, train loss 0.1443, train acc 0.950, test loss 0.9447, test acc 0.764, time 75.4 sec\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cusDhcvrCAzW","colab_type":"text"},"source":["Para efeito de **comparação**, recriamos a rede sem as duas camadas convolucionais (removidas na arquitetura anterior) e sem camadas dilatadas. Dessa forma, podemos observar o ganho das convoluções dilatadas."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ecbavb9fB7C6","colab":{}},"source":["class AlexNet(nn.HybridBlock):\n","    r\"\"\"AlexNet model from the `\"One weird trick...\" `_ paper.\n","\n","    Parameters\n","    ----------\n","    classes : int, default 10\n","        Number of classes for the output layer.\n","    \"\"\"\n","    def __init__(self, classes=10, **kwargs):\n","        super(AlexNet, self).__init__(**kwargs)\n","        with self.name_scope():\n","            self.features = nn.HybridSequential(prefix='')\n","            with self.features.name_scope():\n","                self.features.add(nn.Conv2D(96, kernel_size=11, strides=4, padding=0))                     # entrada: (b, 3, 227, 227) e saida: (b, 96, 55, 55)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.MaxPool2D(pool_size=3, strides=2))                                    # entrada: (b, 96, 55, 55) e saida: (b, 96, 27, 27)\n","                self.features.add(nn.Conv2D(256, kernel_size=5, padding=2))                                # entrada: (b, 96, 27, 27) e saida: (b, 256, 27, 27)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.MaxPool2D(pool_size=3, strides=2))                                    # entrada: (b, 256, 27, 27) e saida: (b, 256, 13, 13)\n","#                 self.features.add(nn.Conv2D(384, kernel_size=3, padding=1))                              # entrada: (b, 256, 13, 13) e saida: (b, 384, 13, 13)\n","#                 self.features.add(nn.BatchNorm())\n","#                 self.features.add(nn.Activation(activation='relu'))\n","#                 self.features.add(nn.Conv2D(384, kernel_size=3, padding=1))                              # entrada: (b, 384, 13, 13) e saida: (b, 384, 13, 13)\n","#                 self.features.add(nn.BatchNorm())\n","#                 self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.Conv2D(256, kernel_size=3, padding=1))                                # entrada: (b, 256, 13, 13) e saida: (b, 256, 13, 13)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.MaxPool2D(pool_size=3, strides=2))                                    # entrada: (b, 256, 13, 13) e saida: (b, 256, 6, 6)\n","                self.features.add(nn.Flatten())                                                            # entrada: (b, 256, 6, 6) e saida: (b, 256*6*6) = (b, 9216)\n","                self.features.add(nn.Dense(4096))                                                          # entrada: (b, 9216) e saida: (b, 4096)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.Dropout(0.5))\n","                self.features.add(nn.Dense(4096))                                                           # entrada: (b, 4096) e saida: (b, 4096)\n","                self.features.add(nn.BatchNorm())\n","                self.features.add(nn.Activation(activation='relu'))\n","                self.features.add(nn.Dropout(0.5))\n","\n","            self.output = nn.Dense(classes)  # entrada: (b, 4096) e saida: (b, 10)\n","\n","    def hybrid_forward(self, F, x):\n","        x = self.features(x)\n","        x = self.output(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"6be53dc6-9279-4866-e48b-7c9dfeb7cfa3","executionInfo":{"status":"ok","timestamp":1562767358954,"user_tz":180,"elapsed":1839114,"user":{"displayName":"Keiller Nogueira","photoUrl":"https://lh5.googleusercontent.com/-OSbB3k7-l84/AAAAAAAAAAI/AAAAAAAAAac/z8WNFFAmye0/s64/photo.jpg","userId":"03938009311988397527"}},"id":"kBR0uN51B7Db","colab":{"base_uri":"https://localhost:8080/","height":403}},"source":["num_epochs, lr, batch_size, wd_lambda = 20, 0.001, 100, 0.0001\n","    \n","net = AlexNet()\n","net.initialize(init.Normal(sigma=0.01), ctx=ctx)\n","\n","# função de custo (ou loss)\n","loss = gloss.SoftmaxCrossEntropyLoss()\n","\n","# carregamento do dado: fashion mnist\n","train_iter, test_iter = load_data_cifar10(batch_size, resize=227)\n","\n","# trainer do gluon\n","trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr, 'wd': wd_lambda})\n","\n","# treinamento e validação via MXNet\n","train_validate(net, train_iter, test_iter, batch_size, trainer, loss, \n","               ctx, num_epochs)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["training on gpu(0)\n","epoch 1, train loss 1.4153, train acc 0.516, test loss 1.3438, test acc 0.540, time 189.2 sec\n","epoch 2, train loss 0.9543, train acc 0.675, test loss 0.8598, test acc 0.710, time 119.1 sec\n","epoch 3, train loss 0.8012, train acc 0.725, test loss 0.7977, test acc 0.725, time 159.1 sec\n","epoch 4, train loss 0.7022, train acc 0.758, test loss 0.7474, test acc 0.743, time 157.6 sec\n","epoch 5, train loss 0.6231, train acc 0.786, test loss 0.9733, test acc 0.686, time 138.7 sec\n","epoch 6, train loss 0.5766, train acc 0.803, test loss 0.6880, test acc 0.763, time 70.4 sec\n","epoch 7, train loss 0.5156, train acc 0.822, test loss 0.6579, test acc 0.780, time 70.4 sec\n","epoch 8, train loss 0.4814, train acc 0.834, test loss 0.6601, test acc 0.771, time 70.8 sec\n","epoch 9, train loss 0.4379, train acc 0.848, test loss 0.7038, test acc 0.764, time 71.0 sec\n","epoch 10, train loss 0.4064, train acc 0.858, test loss 0.6456, test acc 0.785, time 71.1 sec\n","epoch 11, train loss 0.3711, train acc 0.870, test loss 0.6762, test acc 0.790, time 71.2 sec\n","epoch 12, train loss 0.3414, train acc 0.881, test loss 0.6211, test acc 0.796, time 71.5 sec\n","epoch 13, train loss 0.3089, train acc 0.892, test loss 0.6002, test acc 0.805, time 71.2 sec\n","epoch 14, train loss 0.2897, train acc 0.901, test loss 0.5985, test acc 0.807, time 71.5 sec\n","epoch 15, train loss 0.2670, train acc 0.907, test loss 0.6228, test acc 0.804, time 71.0 sec\n","epoch 16, train loss 0.2484, train acc 0.913, test loss 0.5774, test acc 0.815, time 70.5 sec\n","epoch 17, train loss 0.2269, train acc 0.921, test loss 0.5915, test acc 0.811, time 70.8 sec\n","epoch 18, train loss 0.2197, train acc 0.923, test loss 0.6424, test acc 0.795, time 71.1 sec\n","epoch 19, train loss 0.2046, train acc 0.929, test loss 0.7064, test acc 0.798, time 71.2 sec\n","epoch 20, train loss 0.1947, train acc 0.932, test loss 0.6442, test acc 0.811, time 70.6 sec\n"],"name":"stdout"}]}]}