{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S04A01_1 - Introduction to Pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG-mVsVuE0if",
        "colab_type": "text"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ65_lev3RXO",
        "colab_type": "code",
        "outputId": "7f0b7875-6df8-472a-80d0-ecbafd154a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Reinstalling torch with the right CUDA bindings.\n",
        "!pip3 install -U https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install -U https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.1.0 from https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n",
            "  Using cached https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.1.0) (1.16.4)\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.1.0\n",
            "    Uninstalling torch-1.1.0:\n",
            "      Successfully uninstalled torch-1.1.0\n",
            "Successfully installed torch-1.1.0\n",
            "Collecting torchvision==0.3.0 from https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
            "  Using cached https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (4.3.0)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision==0.3.0) (0.46)\n",
            "Installing collected packages: torchvision\n",
            "  Found existing installation: torchvision 0.3.0\n",
            "    Uninstalling torchvision-0.3.0:\n",
            "      Successfully uninstalled torchvision-0.3.0\n",
            "Successfully installed torchvision-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEHmMCjR4PJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic imports.\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwhRUUlc4j23",
        "colab_type": "code",
        "outputId": "bea123f3-d592-4794-d97c-eb731dc64e08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 20,      # Number of epochs.\n",
        "    'n_classes': 10,      # Number of classes.\n",
        "    'lr': 0.05,           # Learning rate.\n",
        "    'weight_decay': 5e-4, # L2 penalty.\n",
        "    'momentum': 0.9,      # Momentum.\n",
        "    'num_workers': 3,     # Number of workers on data loader.\n",
        "    'batch_size': 500,    # Mini-batch size.\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20kc9tHQ59ba",
        "colab_type": "text"
      },
      "source": [
        "# O pacote Torchvision\n",
        "\n",
        "Assim como no MXNet, o subpacote [datasets](https://pytorch.org/docs/stable/torchvision/datasets.html) do [torchvision](https://pytorch.org/docs/stable/torchvision/index.html) do Pytorch possui dataloaders para vários datasets conhecidos da literatura de Visão Computacional. O CIFAR10, já usado previamente no curso, é um desses dataset clássicos de classificação que possuem implementações que podem ser carregados por meio de uma única linha de código. Outros datasets clássicos presentes no torchvision são: MNIST (classificação), Fasion-MNIST (classificação), ImageNet (classificação), VOC (segmentação e detecção), COCO (detecção e captioning), etc. No bloco de código abaixo são carregados os datasets de treino e teste do CIFAR10 e seus respectivos dataloaders.\n",
        "\n",
        "O torchvision também possui outras funções de apoio ao treino de modelos para Deep Learning em imagens, como a definição de modelos (pré-treinados ou não) no subpacote [models](https://pytorch.org/docs/stable/torchvision/models.html) e a possibilidade de realizar pré-processamentos em imagens (incluindo Data Augmentation) por meio de transformações comuns em imagens no subpacote [transforms](https://pytorch.org/docs/stable/torchvision/transforms.html). Essas transformações também são demonstradas no código abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi3Zh8fQ4X_3",
        "colab_type": "code",
        "outputId": "182e8bf5-75e5-4027-a77d-9e28b35cafc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Root directory for the dataset (to be downloaded).\n",
        "root = './'\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Pad(2),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Setting datasets and dataloaders.\n",
        "train_set = datasets.MNIST(root,\n",
        "                           train=True,\n",
        "                           download=True,\n",
        "                           transform=data_transforms)\n",
        "test_set = datasets.MNIST(root,\n",
        "                          train=False,\n",
        "                          download=False,\n",
        "                          transform=data_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_set,\n",
        "                          args['batch_size'],\n",
        "                          num_workers=args['num_workers'],\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(test_set,\n",
        "                         args['batch_size'],\n",
        "                         num_workers=args['num_workers'],\n",
        "                         shuffle=False)\n",
        "\n",
        "# Printing training and testing dataset sizes.\n",
        "print('Size of training set: ' + str(len(train_set)) + ' samples')\n",
        "print('Size of test set: ' + str(len(test_set)) + ' samples')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: 60000 samples\n",
            "Size of test set: 10000 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drOsx-32Ifo1",
        "colab_type": "text"
      },
      "source": [
        "# Programação Orientada a Objetos\n",
        "\n",
        "Faz-se necessário o uso de  [Programação Orientada a Objetos](https://www.tutorialspoint.com/python/python_classes_objects.htm) (POO) à medida que se começa a lidar com tarefas mais específicas tratadas por algoritmos de Deep Learning. Algumas vantagens que POO trazem para a programação de algoritmos de Deep Learning são:\n",
        "\n",
        "1.   Organização do código. Criar uma arquitetura com centenas de camadas podendo conter várias *skip connections* no meio de um bloco de código, como vínhamos fazendo anteriormente, torna o nosso código desorganizado;\n",
        "2.   Reaproveitamento dos códigos básicos do framework (i.e. dataloaders, módulos/camadas de redes predefinidos, funções de perda, etc) para adaptá-los para novas tarefas minimizando a quantidade de código que necessita ser escrita;\n",
        "3.   Códigos escritos em formato POO podem ser reutilizados em outros pontos da aplicação. Arquiteturas como [DenseNets](https://arxiv.org/abs/1608.06993), [ResNets](https://arxiv.org/abs/1512.03385) ou [U-Nets](https://arxiv.org/abs/1505.04597) possuem blocos de código que se repetem, os quais podem ser simplificados com o uso de POO;\n",
        "4.   POO também permite uma melhor modularização do código, de forma que mudanças (i.e. loss function, arquitetura, dataloader, etc) possam ser feitas em módulos isolados sem afetar o resto do código.\n",
        "\n",
        "No caso da implementação de modelos usando classes no Pytorch, normalmente é preciso se preocupar apenas com a implementação de dois métodos: o *\\_\\_init\\_\\_()* e o *forward()*. O *\\_\\_init\\_\\_()* é conhecido como o construtor da classe e define a arquitetura da rede neural que é implementada pela classe, ou seja, é nesse método que se definem as camadas da rede neural. Já o *forward()* serve para definir o fluxo de dados que passará pelas camadas definidas no *\\_\\_init\\_\\_()*. O *forward()* recebe sempre como entrada os dados que passarão pelo modelo e deve retornar o output do modelo após processar esses dados de entrada ao longo de suas camadas. É notável que entre as camadas convolucionais e as camadas Fully Connected, faz-se necessário chamar o método [*view()*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) para linearizar os dados do tensor.\n",
        "\n",
        "Também é preciso lembrar de sempre definir a classe em que o modelo está sendo implementado como subclasse de *nn.Module*, como mostrado abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg1Ci0ZQ4xIK",
        "colab_type": "code",
        "outputId": "8b0e998a-5752-4fb4-b452-6d197924e78d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# LeNet implementation.\n",
        "class LeNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_classes=10):\n",
        "\n",
        "        super(LeNet, self).__init__()\n",
        "        \n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5, padding=0, ),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n",
        "            nn.Conv2d(6, 16, kernel_size=5, padding=0),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n",
        "            nn.Conv2d(16, 120, kernel_size=5, padding=0),\n",
        "            nn.Tanh(),\n",
        "            \n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=120, out_features=84, bias=True),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=84, out_features=num_classes, bias=True)\n",
        "        )       \n",
        "        self.initialize_weights()\n",
        "    \n",
        "    # Function for randomly initializing weights.\n",
        "    def initialize_weights(self):\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1) # Linearizing.\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Instantiating architecture.\n",
        "net = LeNet(args['n_classes']).to(args['device'])\n",
        "\n",
        "# Printing architecture.\n",
        "print(net)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): Tanh()\n",
            "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (4): Tanh()\n",
            "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    (6): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (7): Tanh()\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (1): Tanh()\n",
            "    (2): Linear(in_features=84, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS2l_pqAI0F2",
        "colab_type": "text"
      },
      "source": [
        "# Definindo o otimizador\n",
        "\n",
        "O Pytorch possui vários otimizadores prontos no subpacote [optim](https://pytorch.org/docs/stable/optim.html), desde o SGD básico a otimizadores mais complexos e com taxas de aprendizado por parâmetro como o Adagrad, RMSProp e Adam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_-RN1wH-4bB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.SGD(net.parameters(),\n",
        "                      lr=args['lr'],\n",
        "                      momentum=args['momentum'],\n",
        "                      weight_decay=args['weight_decay'])\n",
        "\n",
        "for state in optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.to(args['device'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVhOWUkWKU4f",
        "colab_type": "text"
      },
      "source": [
        "# Definindo a loss\n",
        "\n",
        "O subpacote [nn](https://pytorch.org/docs/stable/nn.html) possui várias funções de perda para diferentes tarefas (i.e. Cross Entropy, Negative Log Likelihood, loss L1, MSE, Kullback Leibler Divergence, etc) implementadas por padrão.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX_bmN3__LIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().to(args['device'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXhZakGZK_kU",
        "colab_type": "text"
      },
      "source": [
        "# Criando funções para Treino e Teste\n",
        "\n",
        "Iterando sobre os datasets/dataloaders de treino e teste do CIFAR10. Abaixo são implementadas a função *train()* que itera sobre os batches do dataset de treino e atualiza o modelo e a função *test()* que apenas realiza o forward dos dados de teste no modelo e calcula a acurácia no dataset de teste para o modelo no estado atual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCU5Gx9D_6xW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, net, criterion, optimizer, epoch):\n",
        "\n",
        "    # Setting network for training mode.\n",
        "    net.train()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    train_loss = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "\n",
        "        # Obtaining images, labels and paths for batch.\n",
        "        inps, labs = batch_data\n",
        "        \n",
        "        # Casting to cuda variables.\n",
        "        inps = inps.to(args['device'])\n",
        "        labs = labs.to(args['device'])\n",
        "        \n",
        "        # Clears the gradients of optimizer.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forwarding.\n",
        "        outs = net(inps)\n",
        "\n",
        "        # Computing loss.\n",
        "        loss = criterion(outs, labs)\n",
        "\n",
        "        # Computing backpropagation.\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Updating lists.\n",
        "        train_loss.append(loss.data.item())\n",
        "    \n",
        "    train_loss = np.asarray(train_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('[epoch %d], [train loss %.4f +/- %.4f]' % (\n",
        "        epoch, train_loss.mean(), train_loss.std()))\n",
        "    print('--------------------------------------------------------------------')\n",
        "    \n",
        "def test(test_loader, net, criterion, epoch):\n",
        "\n",
        "    # Setting network for evaluation mode.\n",
        "    net.eval()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    test_loss = []\n",
        "    prd_list = []\n",
        "    lab_list = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "\n",
        "        # Obtaining images, labels and paths for batch.\n",
        "        inps, labs = batch_data\n",
        "\n",
        "        # Casting to cuda variables.\n",
        "        inps = inps.to(args['device'])\n",
        "        labs = labs.to(args['device'])\n",
        "\n",
        "        # Forwarding.\n",
        "        outs = net(inps)\n",
        "\n",
        "        # Computing loss.\n",
        "        loss = criterion(outs, labs)\n",
        "        \n",
        "        # Obtaining predictions.\n",
        "        prds = outs.data.max(dim=1)[1].cpu().numpy()\n",
        "        \n",
        "        # Updating lists.\n",
        "        test_loss.append(loss.data.item())\n",
        "        prd_list.append(prds)\n",
        "        lab_list.append(labs.detach().cpu().numpy())\n",
        "    \n",
        "    # Computing accuracy.\n",
        "    acc = metrics.accuracy_score(np.asarray(lab_list).ravel(),\n",
        "                                 np.asarray(prd_list).ravel())\n",
        "    \n",
        "    test_loss = np.asarray(test_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('[epoch %d], [test loss %.4f +/- %.4f], [acc %.4f]' % (\n",
        "        epoch, test_loss.mean(), test_loss.std(), acc))\n",
        "    print('--------------------------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijo07bsTMFMs",
        "colab_type": "text"
      },
      "source": [
        "# Iterando sobre epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU2aYIob_zTu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "72600412-7d5a-4945-c57e-aeb431a0f951"
      },
      "source": [
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "\n",
        "    # Training function.\n",
        "    train(train_loader, net, criterion, optimizer, epoch)\n",
        "\n",
        "    # Computing test loss and metrics.\n",
        "    test(test_loader, net, criterion, epoch)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------\n",
            "[epoch 1], [train loss 1.9660 +/- 0.5083]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 1], [test loss 0.7367 +/- 0.0493], [acc 0.7701]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 2], [train loss 0.3951 +/- 0.1424]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 2], [test loss 0.2270 +/- 0.0295], [acc 0.9340]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 3], [train loss 0.1652 +/- 0.0398]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 3], [test loss 0.1203 +/- 0.0211], [acc 0.9642]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 4], [train loss 0.1027 +/- 0.0209]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 4], [test loss 0.0825 +/- 0.0188], [acc 0.9756]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 5], [train loss 0.0808 +/- 0.0194]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 5], [test loss 0.0686 +/- 0.0149], [acc 0.9798]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 6], [train loss 0.0663 +/- 0.0158]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 6], [test loss 0.0581 +/- 0.0152], [acc 0.9829]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 7], [train loss 0.0608 +/- 0.0154]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 7], [test loss 0.0539 +/- 0.0131], [acc 0.9840]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 8], [train loss 0.0532 +/- 0.0142]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 8], [test loss 0.0480 +/- 0.0114], [acc 0.9860]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 9], [train loss 0.0482 +/- 0.0142]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 9], [test loss 0.0417 +/- 0.0119], [acc 0.9883]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 10], [train loss 0.0445 +/- 0.0112]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 10], [test loss 0.0375 +/- 0.0110], [acc 0.9897]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 11], [train loss 0.0411 +/- 0.0123]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 11], [test loss 0.0388 +/- 0.0110], [acc 0.9886]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 12], [train loss 0.0405 +/- 0.0128]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 12], [test loss 0.0351 +/- 0.0098], [acc 0.9899]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 13], [train loss 0.0371 +/- 0.0113]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 13], [test loss 0.0332 +/- 0.0101], [acc 0.9907]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 14], [train loss 0.0363 +/- 0.0107]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 14], [test loss 0.0308 +/- 0.0096], [acc 0.9918]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 15], [train loss 0.0343 +/- 0.0112]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 15], [test loss 0.0317 +/- 0.0102], [acc 0.9908]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 16], [train loss 0.0327 +/- 0.0100]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 16], [test loss 0.0288 +/- 0.0094], [acc 0.9921]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 17], [train loss 0.0300 +/- 0.0104]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 17], [test loss 0.0284 +/- 0.0086], [acc 0.9919]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 18], [train loss 0.0288 +/- 0.0099]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 18], [test loss 0.0261 +/- 0.0080], [acc 0.9930]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 19], [train loss 0.0289 +/- 0.0092]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 19], [test loss 0.0279 +/- 0.0087], [acc 0.9923]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 20], [train loss 0.0272 +/- 0.0079]\n",
            "--------------------------------------------------------------------\n",
            "--------------------------------------------------------------------\n",
            "[epoch 20], [test loss 0.0245 +/- 0.0079], [acc 0.9936]\n",
            "--------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
