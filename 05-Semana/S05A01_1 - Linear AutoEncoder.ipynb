{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S05A01_1 - Linear AutoEncoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG-mVsVuE0if",
        "colab_type": "text"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEHmMCjR4PJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Basic imports.\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwhRUUlc4j23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 10,      # Number of epochs.\n",
        "    'n_classes': 10,      # Number of classes.\n",
        "    'lr': 0.001,          # Learning rate.\n",
        "    'weight_decay': 1e-5, # L2 penalty.\n",
        "    'num_workers': 3,     # Number of workers on data loader.\n",
        "    'batch_size': 100,    # Mini-batch size.\n",
        "    'print_freq': 1,      # Printing frequency.\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20kc9tHQ59ba",
        "colab_type": "text"
      },
      "source": [
        "# Carregando o  MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi3Zh8fQ4X_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Root directory for the dataset (to be downloaded).\n",
        "root = './'\n",
        "\n",
        "# Transformations over the dataset.\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Setting datasets and dataloaders.\n",
        "train_set = datasets.MNIST(root,\n",
        "                           train=True,\n",
        "                           download=True,\n",
        "                           transform=data_transforms)\n",
        "test_set = datasets.MNIST(root,\n",
        "                          train=False,\n",
        "                          download=False,\n",
        "                          transform=data_transforms)\n",
        "\n",
        "# Setting dataloaders.\n",
        "train_loader = DataLoader(train_set,\n",
        "                          args['batch_size'],\n",
        "                          num_workers=args['num_workers'],\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(test_set,\n",
        "                         args['batch_size'],\n",
        "                         num_workers=args['num_workers'],\n",
        "                         shuffle=False)\n",
        "\n",
        "# Printing training and testing dataset sizes.\n",
        "print('Size of training set: ' + str(len(train_set)) + ' samples')\n",
        "print('Size of test set: ' + str(len(test_set)) + ' samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drOsx-32Ifo1",
        "colab_type": "text"
      },
      "source": [
        "# AutoEncoder Linear\n",
        "\n",
        "Todo AutoEncoder (AE) é composto de uma arquitetura Encoder-Decoder, assim como algumas redes para segmentação semântica que vimos na semana passada (i.e. [U-Nets](https://arxiv.org/pdf/1505.04597.pdf) ou [SegNets](https://arxiv.org/pdf/1511.00561.pdf)). AEs tradicionais não eram treinados usando backpropagation, mas sim com o método de **Stacking** de pares de camadas, caracterizando uma estratégia **gulosa** que ignorava interdependências entre pesos de diferentes camadas. O treinamento dessas redes era feito usando **Stacking** porque o algoritmo de backpropagation que vimos ao longo do curso ainda não era estável o bastante na época que os AEs surgiram na literatura, não conseguindo propagar os erros em redes com mais camadas escondidas. AEs atuais são treinados usando backpropagation assim como as outras arquiteturas modernas.\n",
        "\n",
        "![Linear AE](https://www.dropbox.com/s/vdfhfz6jldmidsj/Linear_AE.png?dl=1)\n",
        "\n",
        "O Encoder em um AE tradicional é composto de camadas lineares que diminuem a dimensionalidade dos dados progressivamente, criando uma região de bottleneck no meio da codificação.\n",
        "\n",
        "![Linear AE Encoder](https://www.dropbox.com/s/fyozex4grkim3ze/Linear_AE_Encoder.png?dl=1)\n",
        "\n",
        "Já as camadas do Decoder num AE recuperam gradualmente a dimensionalidade dos dados, reconstruíndo-o da forma mais fiel possível.\n",
        "\n",
        "![Linear AE Decoder](https://www.dropbox.com/s/t67b2z8bh68ei1q/Linear_AE_Decoder.png?dl=1)\n",
        "\n",
        "Ao se usar AEs Lineares em imagens, faz-se necessário linearizar os dados antes de enviá-los para as camadas Fully Connected da rede, lembrando sempre de recuperar as dimensões da imagem decodificada no fim da rede. Tanto a linearização quanto a recuperação da resolução espacial podem ser feitas usando o método *.view()* dos tensores.\n",
        "\n",
        "Percebe-se que os dados passam de uma dimensionalidade alta na entrada para uma dimensionalidade consideravelmente baixa no bottleneck da rede. Portanto AEs podem ser vistos como métodos de redução de dimensionalidade -- assim como o [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) ou o [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) -- porém treináveis para atuarem em um domínio específico (no nosso caso, em reconstrução dígitos). Assim como outros métodos de redução de dimensionalidade, a intuição dos AEs é que as informações importantes para a reconstrução dos dados sejam codificadas no bottleneck enquanto as informações não importantes sejam ignoradas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e27D-vk8lFgJ",
        "colab_type": "text"
      },
      "source": [
        "# Atividade Prática: AutoEncoder Linear\n",
        "\n",
        "Como mencionado previamente, um AE é uma arquitetura **Encoder-Decoder** com camadas de dimensionalidade simétrica. Vamos definir um AE Linear:\n",
        "\n",
        "1.   Crie um Encoder composto por três camadas FC que reduza gradativamente a dimensionalidade inicial dos dados ($28 \\times 28 = 784$) para apenas 16 dimensões no bottleneck;\n",
        "2.   Crie um Decoder com três camadas FC que receba as 16 dimensões dos dados e gradativamente reconstrua as dimensões originais dos dados de entrada;\n",
        "3.   Implemente o *forward()* passando os dados pelo Encoder e subsequentemente pelo Decoder;\n",
        "4.   Defina a loss function (uma loss de regressão) e o otimizador;\n",
        "5.   Complete as funções *train()* e *test()* [linearizando os dados](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view), fazendo o [casting para o device correto](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to) e passando as imagens linearizadas pelo AE. No fim, deve-se usar o criterion pré-definido para calcular a loss e computar o backpropagation.\n",
        "\n",
        "PS.: as ativações de todas as camadas lineares devem ser ReLU, menos a última camada que deve ter como ativação uma função sigmóide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG8Qnl9pWXSc",
        "colab_type": "text"
      },
      "source": [
        "# Definindo a arquitetura"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg1Ci0ZQ4xIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AutoEncoder implementation.\n",
        "class AutoEncoder(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        \n",
        "        # TO DO: define encoder.\n",
        "        self.encoder = #...\n",
        "        \n",
        "        # TO DO: define decoder.\n",
        "        self.decoder = #...\n",
        "        \n",
        "        self.initialize_weights()\n",
        "    \n",
        "    # Function for randomly initializing weights.\n",
        "    def initialize_weights(self):\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # TO DO: implement forward.\n",
        "        # ...\n",
        "\n",
        "# Instantiating architecture.\n",
        "net = AutoEncoder().to(args['device'])\n",
        "\n",
        "# Printing architecture.\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS2l_pqAI0F2",
        "colab_type": "text"
      },
      "source": [
        "# Definindo o otimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_-RN1wH-4bB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO: defining optimizer.\n",
        "optimizer = # ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVhOWUkWKU4f",
        "colab_type": "text"
      },
      "source": [
        "# Definindo a loss\n",
        "\n",
        "A loss de um AE tradicional pode ser vista como uma loss de regressão, como o MSE ou a Loss L1. Como pode ser visto na imagem abaixo, as classes $y$ das amostras não são usadas no treinamento de um AE, mas apenas o próprio dado de entrada $x$. A loss $\\mathcal{L}$ é computada entre a reconstrução do dado $\\hat{x}$ e $x$.\n",
        "\n",
        "![Loss AE](https://www.dropbox.com/s/84w4yz09da9zxfd/Linear_AE_Loss.png?dl=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX_bmN3__LIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO: defining regression loss.\n",
        "criterion = # ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXhZakGZK_kU",
        "colab_type": "text"
      },
      "source": [
        "# Criando funções para Treino e Teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCU5Gx9D_6xW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training procedure.\n",
        "def train(train_loader, net, criterion, optimizer, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for training mode.\n",
        "    net.train()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    train_loss = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "\n",
        "        # Obtaining images and labels for batch.\n",
        "        inps, labs = batch_data\n",
        "        \n",
        "        # TO DO: Casting to cuda variables and reshaping.\n",
        "        inps = # ...\n",
        "        \n",
        "        # TO DO: Clearing the gradients of optimizer.\n",
        "        # ...\n",
        "        \n",
        "        # TO DO: Forwarding.\n",
        "        outs = # ...\n",
        "\n",
        "        # TO DO: Computing loss.\n",
        "        loss = # ...\n",
        "\n",
        "        # TO DO: Computing backpropagation.\n",
        "        # ...\n",
        "        \n",
        "        # TO DO: Taking step in optimizer.\n",
        "        # ...\n",
        "        \n",
        "        # Updating lists.\n",
        "        train_loss.append(loss.data.item())\n",
        "    \n",
        "    toc = time.time()\n",
        "    \n",
        "    train_loss = np.asarray(train_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('[epoch %d], [train loss %.4f +/- %.4f], [training time %.2f]' % (\n",
        "        epoch, train_loss.mean(), train_loss.std(), (toc - tic)))\n",
        "    print('-------------------------------------------------------------------')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD2mQJjjkSgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing procedure.\n",
        "def test(test_loader, net, criterion, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for evaluation mode.\n",
        "    net.eval()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    test_loss = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(test_loader):\n",
        "\n",
        "        # Obtaining images and labels for batch.\n",
        "        inps, labs = batch_data\n",
        "\n",
        "        # TO DO: Casting to cuda variables and reshaping.\n",
        "        inps = # ...\n",
        "\n",
        "        # TO DO: Forwarding.\n",
        "        outs = # ...\n",
        "\n",
        "        # TO DO: Computing loss.\n",
        "        loss = # ...\n",
        "        \n",
        "        # Updating lists.\n",
        "        test_loss.append(loss.data.item())\n",
        "        \n",
        "        # Plotting.\n",
        "        if i == 0 and epoch % args['print_freq'] == 0:\n",
        "            \n",
        "            fig, ax = plt.subplots(2, 8, figsize=(24, 6))\n",
        "        \n",
        "        if i < 8 and epoch % args['print_freq'] == 0:\n",
        "            \n",
        "            ax[0, i].imshow(inps.view(inps.size(0), 28, 28)[0].detach().cpu().numpy())\n",
        "            ax[0, i].set_yticks([])\n",
        "            ax[0, i].set_xticks([])\n",
        "            ax[0, i].set_title('Image ' + str(i + 1))\n",
        "            \n",
        "            ax[1, i].imshow(outs.view(inps.size(0), 28, 28)[0].detach().cpu().numpy())\n",
        "            ax[1, i].set_yticks([])\n",
        "            ax[1, i].set_xticks([])\n",
        "            ax[1, i].set_title('Reconstructed ' + str(i + 1))\n",
        "            \n",
        "        if i == 8 and epoch % args['print_freq'] == 0:\n",
        "            \n",
        "            plt.show()\n",
        "    \n",
        "    toc = time.time()\n",
        "    \n",
        "    test_loss = np.asarray(test_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('[epoch %d], [test loss %.4f +/- %.4f], [testing time %.2f]' % (\n",
        "        epoch, test_loss.mean(), test_loss.std(), (toc - tic)))\n",
        "    print('-------------------------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijo07bsTMFMs",
        "colab_type": "text"
      },
      "source": [
        "# Iterando sobre epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU2aYIob_zTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "\n",
        "    # Training function.\n",
        "    train(train_loader, net, criterion, optimizer, epoch)\n",
        "\n",
        "    # Computing test loss and metrics.\n",
        "    test(test_loader, net, criterion, epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}