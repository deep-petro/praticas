{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S05A02_2 - Sparse AutoEncoder (Gabarito).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG-mVsVuE0if",
        "colab_type": "text"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEHmMCjR4PJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Basic imports.\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwhRUUlc4j23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 10,      # Number of epochs.\n",
        "    'n_classes': 10,      # Number of classes.\n",
        "    'lr': 0.0005,         # Learning rate.\n",
        "    'weight_decay': 1e-5, # L2 penalty.\n",
        "    'num_workers': 3,     # Number of workers on data loader.\n",
        "    'batch_size': 100,    # Mini-batch size.\n",
        "    'print_freq': 1,      # Printing frequency.\n",
        "    'lambda_s': 1.0,      # Sparsity importance in loss computation.\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20kc9tHQ59ba",
        "colab_type": "text"
      },
      "source": [
        "# Carregando o  MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi3Zh8fQ4X_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Root directory for the dataset (to be downloaded).\n",
        "root = './'\n",
        "\n",
        "# Transformations over the dataset.\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Setting datasets and dataloaders.\n",
        "train_set = datasets.MNIST(root,\n",
        "                           train=True,\n",
        "                           download=True,\n",
        "                           transform=data_transforms)\n",
        "test_set = datasets.MNIST(root,\n",
        "                          train=False,\n",
        "                          download=False,\n",
        "                          transform=data_transforms)\n",
        "\n",
        "# Random subset of training data for small data scenarios.\n",
        "# torch.random.manual_seed(12345)\n",
        "# indices = torch.randperm(len(train_set))[:500]\n",
        "# train_set = data.Subset(train_set, indices)\n",
        "\n",
        "# Setting dataloaders.\n",
        "train_loader = DataLoader(train_set,\n",
        "                          args['batch_size'],\n",
        "                          num_workers=args['num_workers'],\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(test_set,\n",
        "                         args['batch_size'],\n",
        "                         num_workers=args['num_workers'],\n",
        "                         shuffle=False)\n",
        "\n",
        "# Printing training and testing dataset sizes.\n",
        "print('Size of training set: ' + str(len(train_set)) + ' samples')\n",
        "print('Size of test set: ' + str(len(test_set)) + ' samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drOsx-32Ifo1",
        "colab_type": "text"
      },
      "source": [
        "# AutoEncoder Esparso\n",
        "\n",
        "[Regularizações](https://medium.com/datadriveninvestor/l1-l2-regularization-7f1b4fe948f2) são componentes comuns em vários métodos da área de Machine Learning que podem servir como um viés para que o algoritmo dê preferência a soluções mais simples, potencialmente prevenindo overfitting no caso de modelos **overcomplete** e/ou **small data**. A loss de um Sparse AE (SAE) adiciona um termo de regularização $\\mathcal{L}_{s}$ à loss de regressão $\\mathcal{L}_{r}$ de um AE tradicional. Dessa forma, a loss total $\\mathcal{L}_{t}$ é dada por:\n",
        "\n",
        "$\\mathcal{L}_{t}(x, \\hat{x}, z) = \\mathcal{L}_{r}(x, \\hat{x}) + \\lambda_{s} \\mathcal{L}_{s}(z).$\n",
        "\n",
        "Um AE tradicional produz features com ativações consideravelmente densas dos inputs passados a ele, já que, como o objetivo principal é reconstrução, toda informação possível deve ser mantida nas representações latentes da rede.\n",
        "\n",
        "![Dense AE](https://www.dropbox.com/s/nfiix8cfk5g9wue/Sparse_AE_1.png?dl=1)\n",
        "\n",
        "Em contraponto, SAEs produzem representações esparsas dos dados que podem ser utilizadas para realizar [**Sparse Coding**](https://en.wikipedia.org/wiki/Sparse_dictionary_learning), o que tem várias aplicações dentro da área de Machine Learning, incluindo [melhorias na performance de algumas tarefas de classificação](https://arxiv.org/pdf/1312.5663.pdf). A imagem abaixo mostra uma rede com ativações mais esparsas devido à adição de um termo de regularização $\\mathcal{L}_{s}(z)$.\n",
        "\n",
        "![Sparse AE](https://www.dropbox.com/s/rs27590a80srntp/Sparse_AE_2.png?dl=1)\n",
        "\n",
        "Para mais informações sobre **Sparse Coding** (e também outros tópicos interessantes de Machine Learning), um bom material pode ser encontrado nos seguintes vídeos:\n",
        "*   https://www.youtube.com/watch?v=7a0_iEruGoM\n",
        "*   https://www.youtube.com/watch?v=L6qhzWWtqQs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ4zzNo33Loe",
        "colab_type": "text"
      },
      "source": [
        "# Atividade Prática: modificando o AE Convolucional para criar representações esparsas\n",
        "\n",
        "1.   Crie uma função que calcule o componente $\\mathcal{L}_{s}(z)$, recebendo o output do encoder computado pelo AE Convolucional e aplique uma regularização L1 sobre ele: $\\frac{1}{N}\\sum_{i=0}^{N}{|z_{i}|}$;\n",
        "2.   Utilize o fator $\\lambda_{s}$ (*args\\['lambda_s\\']*) para criar uma loss composta $\\mathcal{L}_{t}$ nas funções *train()* e *test()* com a loss de regressão $\\mathcal{L}_{r}$ tradicional do AE, a qual já está implementada. A loss final $\\mathcal{L}_{t}$ é que deve ser usada para computar o backward nas funções de treino e teste;\n",
        "3.   Compare visualmente a densidade de ativações nos feature maps calculados no AE Convolucional denso (implementado na aula passada) e no AE Convolucional esparso.\n",
        "\n",
        "PS.: Compute $\\mathcal{L}_{s}$ usando apenas operações vetoriais, ou seja, sem o uso de um *for*, o que deixaria muito lento o algoritmo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e27D-vk8lFgJ",
        "colab_type": "text"
      },
      "source": [
        "# Definindo a arquitetura"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y7IHYWg2NBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AutoEncoder implementation.\n",
        "class SparseAutoEncoder(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "\n",
        "        super(SparseAutoEncoder, self).__init__()\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=(3, 3), stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(8, 16, kernel_size=(3, 3), stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(16, 32, kernel_size=(3, 3), stride=1, padding=1),\n",
        "        )\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(32, 16, kernel_size=(3, 3), stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=2, padding=1, output_padding=1),\n",
        "            nn.Conv2d(16, 8, kernel_size=(3, 3), stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(8, 8, kernel_size=(3, 3), stride=2, padding=1, output_padding=1),\n",
        "            nn.Conv2d(8, 1, kernel_size=(3, 3), stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        self.initialize_weights()\n",
        "    \n",
        "    # Function for randomly initializing weights.\n",
        "    def initialize_weights(self):\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                \n",
        "    def forward(self, x):\n",
        "        \n",
        "        enc = self.encoder(x)\n",
        "        dec = self.decoder(enc)\n",
        "        return dec, enc\n",
        "\n",
        "# Instantiating architecture.\n",
        "net = SparseAutoEncoder().to(args['device'])\n",
        "\n",
        "# Printing architecture.\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS2l_pqAI0F2",
        "colab_type": "text"
      },
      "source": [
        "# Definindo o otimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_-RN1wH-4bB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(net.parameters(),\n",
        "                       lr=args['lr'],\n",
        "                       weight_decay=args['weight_decay'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVhOWUkWKU4f",
        "colab_type": "text"
      },
      "source": [
        "# Definindo a loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB4XYA1VKIXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparse_loss(tensor):\n",
        "    return torch.mean(torch.abs(tensor))\n",
        "#     return torch.sum(torch.abs(tensor))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX_bmN3__LIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion_reconstruction = nn.MSELoss().to(args['device'])\n",
        "# criterion_reconstruction = nn.L1Loss().to(args['device'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXhZakGZK_kU",
        "colab_type": "text"
      },
      "source": [
        "# Criando funções para Treino e Teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCU5Gx9D_6xW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training procedure.\n",
        "def train(train_loader, net, criterion_reconstruction, optimizer, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for training mode.\n",
        "    net.train()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    train_loss = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "\n",
        "        # Obtaining images and labels for batch.\n",
        "        inps, labs = batch_data\n",
        "        \n",
        "        # Casting to cuda variables and reshaping.\n",
        "        inps = inps.to(args['device'])\n",
        "        \n",
        "        # Clears the gradients of optimizer.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forwarding.\n",
        "        outs, cods = net(inps)\n",
        "\n",
        "        # Computing loss.\n",
        "        loss_rec = criterion_reconstruction(outs, inps)\n",
        "        loss_spr = args['lambda_s'] * sparse_loss(cods)\n",
        "        loss = loss_rec + loss_spr\n",
        "\n",
        "        # Computing backpropagation.\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Updating lists.\n",
        "        train_loss.append((loss_rec.data.item(),\n",
        "                           loss_spr.data.item(),\n",
        "                           loss.data.item()))\n",
        "    \n",
        "    toc = time.time()\n",
        "    \n",
        "    train_loss = np.asarray(train_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('[epoch %d], [train rec loss %.4f +/- %.4f], [train spr loss %.4f +/- %.4f], [training time %.2f]' % (\n",
        "        epoch, train_loss[:,0].mean(), train_loss[:,0].std(), train_loss[:,1].mean(), train_loss[:,1].std(), (toc - tic)))\n",
        "    print('-------------------------------------------------------------------')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD2mQJjjkSgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing procedure.\n",
        "def test(test_loader, net, criterion_reconstruction, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for evaluation mode.\n",
        "    net.eval()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    test_loss = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(test_loader):\n",
        "\n",
        "        # Obtaining images and labels for batch.\n",
        "        inps, labs = batch_data\n",
        "\n",
        "        # Casting to cuda variables and reshaping.\n",
        "        inps = inps.to(args['device'])\n",
        "\n",
        "        # Forwarding.\n",
        "        outs, cods = net(inps)\n",
        "\n",
        "        # Computing loss.\n",
        "        loss_rec = criterion_reconstruction(outs, inps)\n",
        "        loss_spr = args['lambda_s'] * sparse_loss(cods)\n",
        "        loss = loss_rec + loss_spr\n",
        "        \n",
        "        # Updating lists.\n",
        "        test_loss.append((loss_rec.data.item(),\n",
        "                          loss_spr.data.item(),\n",
        "                          loss.data.item()))\n",
        "        \n",
        "        if i == 0 and epoch % args['print_freq'] == 0:\n",
        "            \n",
        "            fig, ax = plt.subplots(10, 8, figsize=(16, 20))\n",
        "        \n",
        "        if i < 8 and epoch % args['print_freq'] == 0:\n",
        "            \n",
        "            ax[0, i].imshow(inps[0, 0].detach().cpu().numpy())\n",
        "            ax[0, i].set_yticks([])\n",
        "            ax[0, i].set_xticks([])\n",
        "            ax[0, i].set_title('Image ' + str(i + 1))\n",
        "            \n",
        "            ax[1, i].imshow(outs[0, 0].detach().cpu().numpy())\n",
        "            ax[1, i].set_yticks([])\n",
        "            ax[1, i].set_xticks([])\n",
        "            ax[1, i].set_title('Reconstructed ' + str(i + 1))\n",
        "            \n",
        "            \n",
        "            for c in range(8):\n",
        "                \n",
        "                ax[2 + c, i].imshow(cods[0, c].detach().cpu().numpy())\n",
        "                ax[2 + c, i].set_yticks([])\n",
        "                ax[2 + c, i].set_xticks([])\n",
        "                ax[2 + c, i].set_title('Bottleneck[' + str(c) + '] ' + str(i + 1))\n",
        "            \n",
        "        if i == 8 and epoch % args['print_freq'] == 0:\n",
        "            \n",
        "            plt.show()\n",
        "    \n",
        "    toc = time.time()\n",
        "    \n",
        "    test_loss = np.asarray(test_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('[epoch %d], [test rec loss %.4f +/- %.4f], [test spr loss %.4f +/- %.4f], [testing time %.2f]' % (\n",
        "        epoch, test_loss[:,0].mean(), test_loss[:,0].std(), test_loss[:,1].mean(), test_loss[:,1].std(), (toc - tic)))\n",
        "    print('-------------------------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijo07bsTMFMs",
        "colab_type": "text"
      },
      "source": [
        "# Iterando sobre epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU2aYIob_zTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "\n",
        "    # Training function.\n",
        "    train(train_loader, net, criterion_reconstruction, optimizer, epoch)\n",
        "\n",
        "    # Computing test loss and metrics.\n",
        "    test(test_loader, net, criterion_reconstruction, epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK8o_-7O2y9s",
        "colab_type": "text"
      },
      "source": [
        "# Usando a representação de bottleneck como features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzguQ6aA2t-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn import ensemble\n",
        "from sklearn import metrics\n",
        "\n",
        "# Evaluating procedure for digit classification.\n",
        "def evaluate(train_loader, test_loader, net):\n",
        "    \n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for evaluation mode.\n",
        "    net.eval()\n",
        "\n",
        "    # Lists for features and labels.\n",
        "    train_samples = []\n",
        "    train_labels = []\n",
        "    \n",
        "    test_samples = []\n",
        "    test_labels = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "            \n",
        "        # Obtaining images and labels for batch.\n",
        "        inps, labs = batch_data\n",
        "\n",
        "        # Casting to cuda variables and reshaping.\n",
        "        inps = inps.to(args['device'])\n",
        "\n",
        "        # Forwarding.\n",
        "        outs, cods = net(inps)\n",
        "        \n",
        "        # Iterating over images in batch.\n",
        "        for b in range(inps.size(0)):\n",
        "            \n",
        "            features = cods[b].detach().cpu().numpy().ravel()\n",
        "            label = labs[b].detach().item()\n",
        "            \n",
        "            train_samples.append(features)\n",
        "            train_labels.append(label)\n",
        "            \n",
        "\n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(test_loader):\n",
        "\n",
        "        # Obtaining images and labels for batch.\n",
        "        inps, labs = batch_data\n",
        "\n",
        "        # Casting to cuda variables and reshaping.\n",
        "        inps = inps.to(args['device'])\n",
        "\n",
        "        # Forwarding.\n",
        "        outs, cods = net(inps)\n",
        "        \n",
        "        # Iterating over images in batch.\n",
        "        for b in range(inps.size(0)):\n",
        "            \n",
        "            features = cods[b].detach().cpu().numpy().ravel()\n",
        "            label = labs[b].detach().item()\n",
        "            \n",
        "            test_samples.append(features)\n",
        "            test_labels.append(label)\n",
        "    \n",
        "    # Transforming lists to numpy arrays.\n",
        "    train_samples = np.asarray(train_samples)\n",
        "    train_labels = np.asarray(train_labels)\n",
        "    \n",
        "    test_samples = np.asarray(test_samples)\n",
        "    test_labels = np.asarray(test_labels)\n",
        "    \n",
        "    \n",
        "    # Classifying using an RF.\n",
        "    clf_rf = ensemble.RandomForestClassifier(n_estimators=50)\n",
        "    clf_rf.fit(train_samples, train_labels)\n",
        "    test_preds_rf = clf_rf.predict(test_samples)\n",
        "    \n",
        "    acc_rf = metrics.accuracy_score(test_labels, test_preds_rf)\n",
        "    \n",
        "    toc = time.time()\n",
        "    \n",
        "    # Using the features to perform classification on a supervised setting.\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('[rf accuracy %.4f], [time %.2f]' % (\n",
        "        acc_rf, (toc - tic)))\n",
        "    print('-------------------------------------------------------------------')\n",
        "    \n",
        "evaluate(train_loader, test_loader, net)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}